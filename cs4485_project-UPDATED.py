# -*- coding: utf-8 -*-
"""CS4485_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/MatthewSchofield25/Weather-Emergency-Application/blob/main/CS4485_Project.ipynb
"""

# Main Steps:
# Data Preprocessing: Tokenize data and remove punctuation. Don't forget to .lower
# NLP: Process Data; Detect frequency of words, n-grams (BOW), TF-IDF, possibly Glove Vectors.
# Model: We can use different models and test their accuracies. LTSM possibly

### DATA PREPROCESSING ###

### DATA IS MISSING. MUST INPUT 'test.csv' AND 'train.csv' MANUALLY. ###

import pandas as pd
import numpy as np

import spacy
from geopy.geocoders import Nominatim

import re
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer

# Sentiment analysis
nltk.download("vader_lexicon")
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.metrics import accuracy_score

# Labels on the data
#id : A unique identifier for each tweet.
#keyword : A particular keyword from the tweet.
#location: The location the tweet was sent from (may be blank).
#text : The text of the tweet.
#target : This denotes whether a tweet is about a real disaster (1) or not (0).

common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest'
                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

ps = PorterStemmer()
lm = WordNetLemmatizer()

X = train.drop(columns=["target"],axis=1)
y = train["target"]

# Load the SpaCy NLP model
nlp = spacy.load('en_core_web_sm')

# Initialize the GeoPy geocoder
geolocator = Nominatim(user_agent='project_app')

def text_cleaning(data):
    return ' '.join(i for i in data.split() if i not in common_words)

def preprocess_data(data):
    '''
    Input: Data to be cleaned.
    Output: Cleaned Data.

    '''
    review =re.sub(r'https?://\S+|www\.\S+|http?://\S+',' ',data) #removal of url
    review =re.sub(r'<.*?>',' ',review) #removal of html tags
    review = re.sub("["
                           u"\U0001F600-\U0001F64F"  # removal of emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+",' ',review)
    review = re.sub('[^a-zA-Z]',' ',review) # filtering out miscellaneous text.
    review = review.lower() # Lowering all the words in text
    review = review.split() # split into a list of words
    review = [lm.lemmatize(words) for words in review if words not in stopwords.words('english')] # Turn words into their stems/roots
    review = [i for i in review if len(i)>2] # Removal of words with length<2
    review = ' '.join(review) # Put back to single string with a space separator
    return review

def sentiment_ana(data):
    sid_obj = SentimentIntensityAnalyzer()
    sentiment_dict = sid_obj.polarity_scores(data)
    return sentiment_dict['compound']

def sentiment_ana_label(data):
    sid_obj = SentimentIntensityAnalyzer()
    sentiment_dict = sid_obj.polarity_scores(data)
    compound_score = sentiment_dict['compound']
    if compound_score >= 0.05:
        return 'positive'
    elif compound_score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

# Define a function to extract location names from a text using SpaCy NER
def extract_locations(text):
    doc = nlp(text)
    return [ent.text for ent in doc.ents if ent.label_ in ['LOC', 'GPE']]

def top_ngrams(data,n,grams):

    if grams == 1:
        count_vec = CountVectorizer(ngram_range=(1,1)).fit(data)
        bow = count_vec.transform(data)
        add_words = bow.sum(axis=0)
        word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]
        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)
    elif grams == 2:
        count_vec = CountVectorizer(ngram_range=(2,2)).fit(data)
        bow = count_vec.transform(data)
        add_words = bow.sum(axis=0)
        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]
        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)
    elif grams == 3:
        count_vec = CountVectorizer(ngram_range=(3,3)).fit(data)
        bow = count_vec.transform(data)
        add_words = bow.sum(axis=0)
        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]
        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)

    return word_freq[:n]

train["Cleaned_text"] = train["text"].apply(preprocess_data)
test["Cleaned_text"] = test["text"].apply(preprocess_data)

train["Cleaned_text"] = train["Cleaned_text"].apply(text_cleaning)
test["Cleaned_text"] = test["Cleaned_text"].apply(text_cleaning)

train["Sentiment"] = train["text"].apply(sentiment_ana)
test["Sentiment"] = test["text"].apply(sentiment_ana)

train["Sentiment_Label"] = train["text"].apply(sentiment_ana_label)
test["Sentiment_Label"] = test["text"].apply(sentiment_ana_label)

train["Location_Test"] = train["text"].apply(extract_locations)


common_words_uni = top_ngrams(train["Cleaned_text"],20,1)
common_words_bi = top_ngrams(train["Cleaned_text"],20,2)
common_words_tri = top_ngrams(train["Cleaned_text"],20,3)

print(common_words_uni)
print(common_words_bi)
print(common_words_tri)

# Just outputs the top words; the n-gram code can also do this

'''
from wordcloud import WordCloud

# Generate the word cloud
wc = WordCloud(background_color='black')
wc.generate(' '.join(test['Cleaned_text']))

# Extract the word frequencies from the word cloud
word_freq = wc.words_

# Print the top 50 words by frequency
top_50_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:50]

for word, freq in top_50_words:
    print(f"{word}: {freq}")

    '''

### OPTIONAL KINDA...NOT SURE IF IT ACTUALLY INCREASED THE PERFORMANCE OF THE MODEL ###
### MIGHT TEST IT LATER ###
'''
common_wordss = ['bsky social','game','good','love','really','alway','well','lot','even','post','said','going','little','right','made',
                'day','much','thing','see','know','let','look','come','got','still','say','think','great','pleas']

def text_cleaning(data):
    return ' '.join(i for i in data.split() if i not in common_wordss)

# Find common words and get rid of words that are unneeded
#train["Cleaned_text"] = train["Cleaned_text"].apply(text_cleaning)
test["Cleaned_text"] = test["Cleaned_text"].apply(text_cleaning)
'''

### OPTIONAL; ADD ONLY IF YOU WANT TO REMOVE THESE POSTS ###

'''
import pandas as pd

# If the posts have these words, remove them from the 'database'; add more for any you want to remove
keywords_to_remove = ['trump', 'elon', 'democrat', 'democrats', 'congress', 'republican', 'musk','maga']

# Create a function to check if any keyword exists in the text
def contains_keywords(text, keywords):
    return any(keyword in text for keyword in keywords)

# Remove the rows with any keywords
test = test[~test['Cleaned_text'].apply(lambda x: contains_keywords(x, keywords_to_remove))]
'''

### CAN DETECT DISASTER TYPE ### Not a predictive model ###

# Array of disaster types (change later)
disaster_keywords = {
    'earthquake': ['earthquake', '#earthquake'],
    'flood': ['flood', '#flood'],
    'fire': ['fire', '#fire'],
    'storm': ['storm', '#storm'],
    'hurricane': ['hurricane', '#hurricane'],
    'tornado': ['tornado', '#tornado'],
    'tsunami': ['tsunami', '#tsunami'],
    'wildfire': ['wildfire', '#wildfire'],
    'drought': ['drought', '#drought'],
    'avalanche': ['avalanche', '#avalanche'],
}

def get_disaster_type(text):
    """ Return the type of disaster based on the tweet's content """
    for disaster, keywords in disaster_keywords.items():
        for keyword in keywords:
            if keyword.lower() in text.lower():
                return disaster
    return 'other'  # Default if no match

# Add a column for disaster type
train['disaster_type'] = train['text'].apply(get_disaster_type)

# Convert the disaster_type column to numeric labels for multi-class classification
disaster_types = train['disaster_type'].unique()
disaster_type_dict = {disaster: idx for idx, disaster in enumerate(disaster_types)}
train['disaster_type_label'] = train['disaster_type'].map(disaster_type_dict)

train.head(100)

### TF_IDF AND LSTM ###

## ACCURACY IS INCONSISTENT ##

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.layers import Embedding,Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.layers import LSTM,Bidirectional,GRU,MaxPooling1D,Conv1D
from tensorflow.keras.layers import Dense
from keras.optimizers import Adam,SGD
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import *
n_epoch = 30
from sklearn.model_selection import train_test_split

def encoding(train_data,test_data):
    tfidf = TfidfVectorizer(
          ngram_range=(1, 1), use_idf=True, smooth_idf=True, sublinear_tf=True
    )
    tf_df_train = tfidf.fit_transform(train_data).toarray()
    train_df = pd.DataFrame(tf_df_train,columns=tfidf.get_feature_names_out())
    tf_df_test = tfidf.transform(test_data).toarray()
    test_df = pd.DataFrame(tf_df_test,columns=tfidf.get_feature_names_out())

    return train_df,test_df

x_final,x_test_final = encoding(train["Cleaned_text"],test["Cleaned_text"])
y_final = np.array(y)

x_final.shape,y_final.shape,x_test_final.shape

# Dividing the data into training, validation and testing
x_train, x_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.1, random_state=42, stratify = y_final)
X_train, x_valid, Y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify = y_train)
x_test_final = x_test_final

embedding_feature_vector = 200 # Since we used glove vector embedding of dim 200.
model = Sequential()
model.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))  # Input layer with TF-IDF features
model.add(Dropout(0.35))  # Dropout layer for regularization
model.add(Dense(128, activation='relu'))  # Hidden layer
model.add(Dropout(0.35))  # Dropout layer for regularization
model.add(Dense(32, activation='relu'))  # Hidden layer
model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())


early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1,
                           mode='min', restore_best_weights=True)

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5,
                              verbose=1, mode='min')

history = model.fit(X_train,Y_train,validation_data=(x_valid,y_valid),callbacks=[reduce_lr,early_stop],epochs=n_epoch,batch_size= 64)

predictions = model.predict(x_valid)

# Convert probabilities to binary values (0 or 1)
binary_predictions = (predictions > 0.5).astype(int)

# Print the first 10 predictions
print("First 10 predictions on validation set (1 = disaster, 0 = not disaster):")
print(binary_predictions[:10])

# Actual prediction probabilities (between 0 and 1)
print("First 10 raw prediction probabilities:")
print(predictions[:10])

accuracy = accuracy_score(y_valid, binary_predictions)

# Output the overall accuracy
print(f"Validation accuracy: {accuracy * 100:.2f}%")

predictions = model.predict(x_test_final)

# Convert probabilities to binary values (0 or 1)
binary_predictions = (predictions > 0.5).astype(int)

print("First 10 predictions on test set (1 = disaster, 0 = not disaster):")
print(binary_predictions[:10])

accuracy_test = accuracy_score(test['target'], binary_predictions)

# Testing accuracy
print(f"Test set accuracy: {accuracy_test * 100:.2f}%")

# F1 score for the test set
f1_test = f1_score(test['target'], binary_predictions)

print(f"Test set F1 score: {f1_test:.2f}")

test['prediction'] = binary_predictions

### MOVED TO END OF K-MEANS ALGO
#test.to_csv('BlueSkyTestPredictions.csv', index=False)

### K-Means Clustering Function; Clusters similar disaster posts with each other so we can detect ongoing situations. Kinda simple for now, will update later.

##### UPDATE: Updated K-means with location and time; location is not filtered for states. I'll keep working on that and see if it can work.

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from scipy.sparse import hstack, csr_matrix
import pandas as pd

# TF-IDF Encoding Function
def encoding(train_data, test_data):
    tfidf = TfidfVectorizer(
        ngram_range=(1, 1), use_idf=True, smooth_idf=True, sublinear_tf=True
    )
    tf_df_train = tfidf.fit_transform(train_data).toarray()
    train_df = pd.DataFrame(tf_df_train, columns=tfidf.get_feature_names_out())
    tf_df_test = tfidf.transform(test_data).toarray()
    test_df = pd.DataFrame(tf_df_test, columns=tfidf.get_feature_names_out())

    return train_df, test_df, tfidf


disaster_posts = test[test['prediction'] == 1].copy()
disaster_posts['original_index'] = disaster_posts.index

test['Location_Test'] = test['Location_Test'].apply(
    lambda x: ', '.join(x) if isinstance(x, list) else x
)

from sklearn.preprocessing import OneHotEncoder

disaster_posts['Location_Test'] = disaster_posts['Location_Test'].fillna("unknown").astype(str)
location_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)
location_encoded = location_encoder.fit_transform(disaster_posts[['Location_Test']])

# Check if there are any disaster posts
if not disaster_posts.empty:
    disaster_posts['text'] = disaster_posts['Cleaned_text']
    disaster_posts['timeposted'] = pd.to_datetime(disaster_posts['timeposted'])

    # Get time
    disaster_posts['year'] = disaster_posts['timeposted'].dt.year
    disaster_posts['month'] = disaster_posts['timeposted'].dt.month
    disaster_posts['day'] = disaster_posts['timeposted'].dt.day
    disaster_posts['hour'] = disaster_posts['timeposted'].dt.hour

    # TF-IDF vectorization
    _, disaster_tfidf_matrix, tfidf = encoding(train["Cleaned_text"], disaster_posts['text'])

    # Scale time features
    date_features = disaster_posts[['year', 'month', 'day', 'hour']].values
    date_features_scaled = StandardScaler().fit_transform(date_features)

    combined_features = hstack([
        disaster_tfidf_matrix,
        csr_matrix(date_features_scaled),
        location_encoded
    ])

    # Cluster if enough data
    if combined_features.shape[0] > 1 and combined_features.shape[1] > 1:
        num_clusters = min(600, combined_features.shape[0])

        kmeans = KMeans(n_clusters=num_clusters, random_state=42)
        kmeans.fit(combined_features)

        disaster_posts = disaster_posts.reset_index(drop=True)
        test.loc[test['prediction'] == 1, 'cluster'] = kmeans.labels_


        for cluster_id, group in test[test['cluster'].notnull()].groupby('cluster'):
            top_locs = group['Location_Test'].value_counts().head(3)
            print(f"Cluster {cluster_id} - Posts:")
            print(group['Cleaned_text'].head(5).to_string(index=False))
            print(top_locs.to_string(), "\n")

        # Initialize 'cluster'
        test['cluster'] = None

        # Write the cluster labels into test at the right rows
        test.loc[disaster_posts['original_index'], 'cluster'] = disaster_posts['cluster'].values

        test.to_csv('BlueSkyTestPredictions.csv', index=False)

    else:
        print("Not enough data to perform clustering.")
else:
    print("No disaster posts found for clustering.")