# -*- coding: utf-8 -*-
"""Copy of Testing_CS4485_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HwF35Xa6knR5EldrmGEj8YeB3hwIXsZ0
"""

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
import spacy
nlp = spacy.load("en_core_web_sm")

nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('wordnet')

#loading data
train = pd.read_csv("train.csv")
train.rename(columns={"Text": "text", "Label": "target"}, inplace=True)
train["target"] = train["target"].astype(int)

test = pd.read_csv("TestDataBluesky.csv")
test.rename(columns={"post_text": "text"}, inplace=True)

#preprocessing
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest',
                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']

def preprocess_data(data):
    data = re.sub(r'https?://\S+|www\.\S+', ' ', data)
    data = re.sub(r'<.*?>', ' ', data)
    data = re.sub(r'[^a-zA-Z]', ' ', data)
    data = data.lower().split()
    data = [lemmatizer.lemmatize(word, wordnet.VERB) for word in data if word not in stop_words and word not in common_words]
    return ' '.join(data)

train["Cleaned_text"] = train["text"].apply(preprocess_data)
test["Cleaned_text"] = test["text"].apply(preprocess_data)

#sentiment analysis
sia = SentimentIntensityAnalyzer()
train["Sentiment"] = train["text"].apply(lambda x: sia.polarity_scores(x)["compound"])
test["Sentiment"] = test["text"].apply(lambda x: sia.polarity_scores(x)["compound"])

#detecting relevance
vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))
X_tfidf = vectorizer.fit_transform(train["Cleaned_text"])
y_tfidf = train["target"]

X_train_tfidf, X_val_tfidf, y_train_tfidf, y_val_tfidf = train_test_split(X_tfidf, y_tfidf, test_size=0.2, random_state=42)
relevance_model = LogisticRegression()
relevance_model.fit(X_train_tfidf, y_train_tfidf)

#evaluating
y_val_pred = relevance_model.predict(X_val_tfidf)
val_accuracy = accuracy_score(y_val_tfidf, y_val_pred)
val_f1 = f1_score(y_val_tfidf, y_val_pred)

print(f"Relevance Classifier Accuracy: {val_accuracy:.4f}")
print(f"Relevance Classifier F1 Score: {val_f1:.4f}")
print("\nClassification Report:")
print(classification_report(y_val_tfidf, y_val_pred))

X_test_tfidf = vectorizer.transform(test["Cleaned_text"])
test_probs = relevance_model.predict_proba(X_test_tfidf)[:, 1]
test["Relevant"] = (test_probs > 0.7).astype(int)


#fitler the relevant post
relevant_posts = test[test["Relevant"] == 1].copy()

#disaster categorization
disaster_keywords = {
    "Earthquake": ["earthquake", "quake", "seismic", "richter", "aftershock", "tremor"],
    "Wildfire": ["wildfire", "bushfire", "forest fire", "firestorm", "blaze"],
    "Hurricane": ["hurricane", "cyclone", "typhoon", "storm surge", "tropical storm"],
    "Flood": ["flood", "flash flood", "heavy rain", "overflow", "dam failure"],
    "Tornado": ["tornado", "twister", "funnel cloud", "storm"],
    "Tsunami": ["tsunami", "seismic wave", "ocean surge"],
    "Volcano": ["volcano", "eruption", "lava", "ash cloud", "magma"],
    "Landslide": ["landslide", "mudslide", "rockfall", "avalanche"],
    "Drought": ["drought", "water shortage", "dry spell", "desertification"],
    "Blizzard": ["blizzard", "snowstorm", "ice storm", "whiteout"]
}
disaster_categories = {k: i for i, k in enumerate(list(disaster_keywords.keys()) + ["Other"])}

def categorize_disaster(text):
    text = text.lower()
    for disaster, keywords in disaster_keywords.items():
        if any(word in text for word in keywords):
            return disaster
    return "Other"

train["category"] = train["Cleaned_text"].apply(categorize_disaster)
train["category_label"] = train["category"].map(disaster_categories)

def extract_location(text):
    doc = nlp(text)
    locations = [ent.text for ent in doc.ents if ent.label_ == "GPE"]
    return ", ".join(locations) if locations else "None"

#LSTM prep
MAX_NB_WORDS = 5000
MAX_SEQUENCE_LENGTH = 100
EMBEDDING_DIM = 100

keras_tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)
keras_tokenizer.fit_on_texts(train["Cleaned_text"])

X_train_seq = keras_tokenizer.texts_to_sequences(train["Cleaned_text"])
X_test_seq = keras_tokenizer.texts_to_sequences(relevant_posts["Cleaned_text"])

X_train_seq = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)
X_test_seq = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)

y_train_cat = to_categorical(train["category_label"], num_classes=len(disaster_categories))

#LSTM model
model = Sequential([
    Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),
    SpatialDropout1D(0.2),
    LSTM(64, return_sequences=True),
    LSTM(32),
    Dense(len(disaster_categories), activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
X_lstm_train, X_lstm_val, y_lstm_train, y_lstm_val = train_test_split(
    X_train_seq, y_train_cat, test_size=0.2, random_state=42
)

model.fit(X_lstm_train, y_lstm_train, epochs=5, batch_size=32, validation_data=(X_lstm_val, y_lstm_val))

#predicting
predictions = model.predict(X_test_seq)
predicted_categories = predictions.argmax(axis=1)
reverse_category_map = {v: k for k, v in disaster_categories.items()}
relevant_posts["Predicted_Disaster_Type"] = [reverse_category_map[i] for i in predicted_categories]
relevant_posts["Location"] = relevant_posts["text"].apply(extract_location)

#final output
print(relevant_posts[["text", "Predicted_Disaster_Type"]].head(30))
relevant_posts.to_csv("Bluesky_Disaster_Predictions_With_Relevance.csv", index=False)