# -*- coding: utf-8 -*-
"""Copy of Testing_CS4485_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HwF35Xa6knR5EldrmGEj8YeB3hwIXsZ0
"""
import asyncio
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
import spacy
import pyodbc
import os

driver = '{ODBC Driver 18 for SQL Server}'
server = os.getenv("DATABASE_SERVER")
database = os.getenv("DATABASE_NAME")
username = os.getenv("DATABASE_USERNAME")
password = os.getenv("DATABASE_PASSWORD")

nlp = spacy.load("en_core_web_sm")

nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('wordnet')

#preprocessing
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest',
                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']
disaster_keywords = {
        "Earthquake": ["earthquake", "quake", "seismic", "richter", "aftershock", "tremor"],
        "Wildfire": ["wildfire", "bushfire", "forest fire", "firestorm", "blaze"],
        "Hurricane": ["hurricane", "cyclone", "typhoon", "storm surge", "tropical storm"],
        "Flood": ["flood", "flash flood", "heavy rain", "overflow", "dam failure"],
        "Tornado": ["tornado", "twister", "funnel cloud", "storm"],
        "Tsunami": ["tsunami", "seismic wave", "ocean surge"],
        "Volcano": ["volcano", "eruption", "lava", "ash cloud", "magma"],
        "Landslide": ["landslide", "mudslide", "rockfall", "avalanche"],
        "Drought": ["drought", "water shortage", "dry spell", "desertification"],
        "Blizzard": ["blizzard", "snowstorm", "ice storm", "whiteout"]
    }

def preprocess_data(data):
        data = re.sub(r'https?://\S+|www\.\S+', ' ', data)
        data = re.sub(r'<.*?>', ' ', data)
        data = re.sub(r'[^a-zA-Z]', ' ', data)
        data = data.lower().split()
        data = [lemmatizer.lemmatize(word, wordnet.VERB) for word in data if word not in stop_words and word not in common_words]
        return ' '.join(data)

def categorize_disaster(text):
    text = text.lower()
    for disaster, keywords in disaster_keywords.items():
        if any(word in text for word in keywords):
            return disaster
    return "Other"

def extract_location(text):
    doc = nlp(text)
    locations = [ent.text for ent in doc.ents if ent.label_ == "GPE"]
    return ", ".join(locations) if locations else "None"

#loading data, need to convert to pydobc
async def load_data_train():
    train = pd.read_csv("train.csv")
    train.rename(columns={"Text": "text", "Label": "target"}, inplace=True)
    train["target"] = train["target"].astype(int)
    return train

async def load_data_test():
    try:
        db = pyodbc.connect(f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}')
    except Exception as e:
        print(f"Error connecting to SQL Server: {e}")
        return None
    print("Sql server connection successful")
    cursor = db.cursor()
    cursor.execute("SELECT * FROM Bluesky_Posts")
    results = cursor.fetchall()
    columns = [column[0] for column in cursor.description]
    test = pd.DataFrame.from_records(results, columns=columns)
    test.rename(columns={"post_text": "text"}, inplace=True)
    return test

#loads Vanessa's model output into the table LSTM_Posts, 4_2
async def load_LSTM_data():
    try:
        conn = pyodbc.connect(f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}')
    except Exception as e:
        print(f"Error connecting to SQL Server: {e}")
        return None
    print("SQL server connection successful")
    cursor = conn.cursor()
    for index, row in test2.iterrows():
        cursor.execute(
            "INSERT INTO LSTM_Posts (post_uri, post_author, post_author_display, post_text, timeposted, sentiment_score, keyword, location, cleaned_text, category, sentiment_label, prediction) "
            "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)", 
            row.post_uri, row.post_author, row.post_author_display, row.text, row.timeposted, row.sentiment_score, row.keyword, row.location, row.cleaned_text, row.category, row.sentiment_label, row.prediction
        )
    conn.commit()
    cursor.close()
    conn.close()
    return "Data inserted into Posts" 

async def main() -> None:
    try:
        train = await load_data_train() # train data will always remain the same
        test = await load_data_test() 
    except Exception as e:
        print(f"Error fetching posts: {e}")
    
    #4_2, renamed variables to lowercase for consistency with database
    train["cleaned_text"] = train["text"].apply(preprocess_data)
    test["cleaned_text"] = test["text"].apply(preprocess_data)

    #sentiment analysis
    sia = SentimentIntensityAnalyzer()
    train["sentiment_score"] = train["text"].apply(lambda x: sia.polarity_scores(x)["compound"])
    test["sentiment_score"] = test["text"].apply(lambda x: sia.polarity_scores(x)["compound"])

    #detecting relevance
    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))
    X_tfidf = vectorizer.fit_transform(train["cleaned_text"])
    y_tfidf = train["target"]

    X_train_tfidf, X_val_tfidf, y_train_tfidf, y_val_tfidf = train_test_split(X_tfidf, y_tfidf, test_size=0.2, random_state=42)
    relevance_model = LogisticRegression()
    relevance_model.fit(X_train_tfidf, y_train_tfidf)

    #evaluating
    y_val_pred = relevance_model.predict(X_val_tfidf)
    val_accuracy = accuracy_score(y_val_tfidf, y_val_pred)
    val_f1 = f1_score(y_val_tfidf, y_val_pred)

    print(f"Relevance Classifier Accuracy: {val_accuracy:.4f}")
    print(f"Relevance Classifier F1 Score: {val_f1:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_val_tfidf, y_val_pred))

    X_test_tfidf = vectorizer.transform(test["cleaned_text"])
    test_probs = relevance_model.predict_proba(X_test_tfidf)[:, 1]
    test["Relevant"] = (test_probs > 0.7).astype(int)


    #fitler the relevant post
    relevant_posts = test[test["Relevant"] == 1].copy()

    #disaster categorization
    disaster_categories = {k: i for i, k in enumerate(list(disaster_keywords.keys()) + ["Other"])}
    train["category"] = train["cleaned_text"].apply(categorize_disaster)
    train["category_label"] = train["category"].map(disaster_categories)

    #LSTM prep
    MAX_NB_WORDS = 5000
    MAX_SEQUENCE_LENGTH = 100
    EMBEDDING_DIM = 100

    keras_tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)
    keras_tokenizer.fit_on_texts(train["cleaned_text"])

    X_train_seq = keras_tokenizer.texts_to_sequences(train["cleaned_text"])
    X_test_seq = keras_tokenizer.texts_to_sequences(relevant_posts["cleaned_text"])

    X_train_seq = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)
    X_test_seq = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)

    y_train_cat = to_categorical(train["category_label"], num_classes=len(disaster_categories))

    #LSTM model
    model = Sequential([
        Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),
        SpatialDropout1D(0.2),
        LSTM(64, return_sequences=True),
        LSTM(32),
        Dense(len(disaster_categories), activation='softmax')
    ])

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    X_lstm_train, X_lstm_val, y_lstm_train, y_lstm_val = train_test_split(
        X_train_seq, y_train_cat, test_size=0.2, random_state=42
    )

    model.fit(X_lstm_train, y_lstm_train, epochs=5, batch_size=32, validation_data=(X_lstm_val, y_lstm_val))

    #predicting
    predictions = model.predict(X_test_seq)
    predicted_categories = predictions.argmax(axis=1)
    reverse_category_map = {v: k for k, v in disaster_categories.items()}
    #4_2, relabled to "category" for consistency with database
        #relevant_posts["Predicted_Disaster_Type"] = [reverse_category_map[i] for i in predicted_categories]
    relevant_posts["category"] = [reverse_category_map[i] for i in predicted_categories]
    relevant_posts["location"] = relevant_posts["text"].apply(extract_location)

    #final output
    print(relevant_posts[["text", "Predicted_Disaster_Type"]].head(30))
    relevant_posts.to_csv("Bluesky_Disaster_Predictions_With_Relevance.csv", index=False)

    #connect to Vanessa's model, 4_2
    #Vanessa receives columns: 
    #post_uri, post_author, post_author_display, text, timeposted, sentiment_score, keyword, location, cleaned_text, category
    test2 = relevant_posts.copy()                                   # test2 will be used to test Vanessa's LSTM model       
    test2["sentiment_label"] = None                                 # create additional columns to be filled   
    test2["prediction"] = None
    #Vanessa outputs:
    #post_uri, post_author, post_author_display, post_text, timeposted, sentiment_score, keyword, location, cleaned_text, category, sentiment_label, prediction	

    ## fixme: insert Vanessa's model here ##

    # connect to LSTM_Posts, then insert posts. 4_2
    try:
        loadSuccess = await load_LSTM_data()
        print(loadSuccess)                                  # for debugging, prints when successfully inserted into LSTM_Posts
    except Exception as e:
        print(f"Error inserting into LSTM_Posts: {e}")


if __name__ == '__main__':
    asyncio.run(main())
