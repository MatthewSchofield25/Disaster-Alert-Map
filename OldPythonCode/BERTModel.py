# -*- coding: utf-8 -*-
"""Copy of BERTDownload

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QgLu0NqeQn1-44_mDPRFBWqAY_NW7R3r
"""

!pip install transformers
!pip install torch
!pip install scikit-learn
!pip install nltk
!pip install spacy

import nltk
nltk.download('stopwords')
nltk.download('vader_lexicon')
nltk.download('wordnet')

import spacy
nlp = spacy.load("en_core_web_sm")

import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from tqdm.auto import tqdm
from sklearn.metrics import classification_report
import time
import os
import re
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
import ast

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

#preprocessing
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest',
                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']
disaster_keywords = {
        "Earthquake": ["earthquake", "quake", "seismic", "richter", "aftershock", "tremor"],
        "Wildfire": ["wildfire", "bushfire", "forest fire", "firestorm", "blaze"],
        "Hurricane": ["hurricane", "cyclone", "typhoon", "storm surge", "tropical storm"],
        "Flood": ["flood", "flash flood", "heavy rain", "overflow", "dam failure"],
        "Tornado": ["tornado", "twister", "funnel cloud", "storm"],
        "Tsunami": ["tsunami", "seismic wave", "ocean surge"],
        "Volcano": ["volcano", "eruption", "lava", "ash cloud", "magma"],
        "Landslide": ["landslide", "mudslide", "rockfall", "avalanche"],
        "Drought": ["drought", "water shortage", "dry spell", "desertification"],
        "Blizzard": ["blizzard", "snowstorm", "ice storm", "whiteout"],
        "Other": []
    }

def preprocess_data(data):
    if not isinstance(data, str):
        data = str(data)  # Convert to string if not already
    data = re.sub(r'https?://\S+|www\.\S+', ' ', data)
    data = re.sub(r'<.*?>', ' ', data)
    data = re.sub(r'[^a-zA-Z]', ' ', data)
    data = data.lower().split()
    data = [lemmatizer.lemmatize(word, wordnet.VERB) for word in data if word not in stop_words and word not in common_words]
    return ' '.join(data)


def load_crisislex_data(folder_path, sample_fraction=0.2, target_size=None):
    dfs = []

    for filename in os.listdir(folder_path):
        if filename.endswith('.csv'):
            file_path = os.path.join(folder_path, filename)
            try:
                df = pd.read_csv(file_path)

                df.columns = df.columns.str.strip()  # remove spaces
                if 'Tweet Text' in df.columns and 'Informativeness' in df.columns:
                    df = df.rename(columns={"Tweet Text": "text", "Informativeness": "label"})
                    dfs.append(df[['text', 'label']])
                else:
                    print(f"Skipping {filename}: missing 'Tweet Text' or 'Informativeness' columns.")
            except Exception as e:
                print(f"Skipping {filename}: {e}")

    if not dfs:
        raise ValueError(f"No valid CSVs loaded from {folder_path}.")

    crisis_df = pd.concat(dfs, axis=0).reset_index(drop=True)
    print(f"Loaded {len(crisis_df)} total tweets from CrisisLex.")

    # Determine Relevancy based on label
    crisis_df['Relevancy'] = crisis_df['label'].apply(lambda x: 1 if 'Related' in x else 0)

    # Filter only relevant posts
    crisis_df = crisis_df[crisis_df['Relevancy'] == 1]
    print(f"After filtering, {len(crisis_df)} tweets marked as relevant.")

    # Soft sample
    if target_size:
        crisis_sample = crisis_df.sample(n=target_size, random_state=42)
    else:
        crisis_sample = crisis_df.sample(frac=sample_fraction, random_state=42)

    print(f"After soft sampling: {len(crisis_sample)} tweets selected.")

    # Preprocessing
    crisis_sample["cleaned_text"] = crisis_sample["text"].apply(preprocess_data)
    crisis_sample["category"] = "Other"  # Still default 'Other'
    crisis_sample["target"] = 1  # Your model expects 'target' = 1 (relevant)

    return crisis_sample


# Load datasets
train_df1 = pd.read_csv("train.csv").rename(columns={"Text": "text", "Label": "target"})
train_df2 = pd.read_csv("Bluesky_Training_Labeled_1000.csv", encoding="latin1").rename(columns={"post_text": "text", "Label": "target"})
train_df = pd.concat([train_df1, train_df2], ignore_index=True)

# Load CrisisLex Data
crisis_sample = load_crisislex_data(
    folder_path="/content/CrisisLexT26Labeled",  # e.g., "./CrisisLexT26"
    sample_fraction=0.2  # OR pick target_size=500
)

#Merge it into training
train_df = pd.concat([train_df, crisis_sample], axis=0).reset_index(drop=True)

# Disaster keywords
disaster_keywords = {
    "Earthquake": ["earthquake", "quake", "seismic", "richter", "aftershock", "tremor"],
    "Wildfire": ["wildfire", "bushfire", "forest fire", "firestorm", "blaze"],
    "Hurricane": ["hurricane", "cyclone", "typhoon", "storm surge", "tropical storm"],
    "Flood": ["flood", "flash flood", "heavy rain", "overflow", "dam failure"],
    "Tornado": ["tornado", "twister", "funnel cloud", "storm"],
    "Tsunami": ["tsunami", "seismic wave", "ocean surge"],
    "Volcano": ["volcano", "eruption", "lava", "ash cloud", "magma"],
    "Landslide": ["landslide", "mudslide", "rockfall", "avalanche"],
    "Drought": ["drought", "water shortage", "dry spell", "desertification"],
    "Blizzard": ["blizzard", "snowstorm", "ice storm", "whiteout"],
}
category_map = list(disaster_keywords.keys()) + ["Other"]
cat_to_idx = {c: i for i, c in enumerate(category_map)}

def categorize_disaster(text):
    text = text.lower()
    for disaster, keywords in disaster_keywords.items():
        if any(word in text for word in keywords):
            return disaster
    return "Other"

train_df["category"] = train_df["text"].apply(categorize_disaster)
train_df["category_label"] = train_df["category"].map(cat_to_idx)

# Balance dataset
rare_classes = train_df["category"].value_counts()[train_df["category"].value_counts() < 10].index
train_df = train_df[~train_df["category"].isin(rare_classes)]
df_majority = train_df[train_df["category"] == "Other"]
df_minority = train_df[train_df["category"] != "Other"]
target_size = df_minority["category"].value_counts().max()
df_other_down = resample(df_majority, replace=False, n_samples=target_size, random_state=42)
upsampled_minority = [resample(g, replace=True, n_samples=target_size, random_state=42) if len(g) < target_size else g for _, g in df_minority.groupby("category")]
train_df = pd.concat([df_other_down] + upsampled_minority)
train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Tokenizer and dataset
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=True)

train_texts, val_texts, train_rel, val_rel, train_cat, val_cat = train_test_split(
    train_df["text"].tolist(),
    train_df["target"].tolist(),
    train_df["category_label"].tolist(),
    test_size=0.2,
    stratify=train_df["category_label"]
)

train_encodings = tokenizer(train_texts, truncation=True, padding="max_length", max_length=128)
val_encodings = tokenizer(val_texts, truncation=True, padding="max_length", max_length=128)

class DisasterDataset(Dataset):
    def __init__(self, encodings, relevance, disaster):
        self.input_ids = encodings["input_ids"]
        self.attention_mask = encodings["attention_mask"]
        self.relevance = relevance
        self.disaster = disaster

    def __len__(self):
        return len(self.relevance)

    def __getitem__(self, idx):
        return {
            "input_ids": torch.tensor(self.input_ids[idx]),
            "attention_mask": torch.tensor(self.attention_mask[idx]),
            "relevance": torch.tensor(self.relevance[idx], dtype=torch.float),
            "disaster": torch.tensor(self.disaster[idx], dtype=torch.long)
        }

train_ds = DisasterDataset(train_encodings, train_rel, train_cat)
val_ds = DisasterDataset(val_encodings, val_rel, val_cat)
train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=64)

# Model
class MultiTaskDisasterModel(nn.Module):
    def __init__(self, base_model="vinai/bertweet-base", num_disaster_classes=11):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(base_model)
        hidden = self.encoder.config.hidden_size
        self.relevance_head = nn.Linear(hidden, 1)
        self.disaster_head = nn.Linear(hidden, num_disaster_classes)

    def forward(self, input_ids, attention_mask):
        x = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]
        return {
            "relevance": self.relevance_head(x),
            "disaster_type": self.disaster_head(x)
        }

model = MultiTaskDisasterModel().to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

# Timing
start_training_time = time.time()

num_epochs = 1
for epoch in range(num_epochs):
    print(f"\n=== Epoch {epoch+1}/{num_epochs} ===")
    start_epoch_time = time.time()

    model.train()
    running_loss = 0.0
    train_steps = 0

    train_true_rel, train_pred_rel = [], []
    train_true_cat, train_pred_cat = [], []

    pbar = tqdm(train_loader, desc=f"Training Epoch {epoch+1}", leave=True, dynamic_ncols=True)

    for batch in pbar:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        relevance = batch["relevance"].unsqueeze(1).to(device)
        disaster = batch["disaster"].to(device)

        optimizer.zero_grad()
        out = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = (
            F.binary_cross_entropy_with_logits(out["relevance"], relevance) +
            F.cross_entropy(out["disaster_type"], disaster)
        )
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        train_steps += 1

        #track predictions
        logits = out["relevance"].squeeze()
        train_pred_rel.extend((logits > 0.5).int().cpu().tolist())
        train_pred_cat.extend(torch.argmax(out["disaster_type"], dim=1).cpu().tolist())
        train_true_rel.extend(relevance.cpu().int().tolist())
        train_true_cat.extend(disaster.cpu().tolist())

        # Update progress bar with current loss
        pbar.set_postfix({"batch_loss": loss.item()})

    # after training loop
    print("\n+ Training Metrics (Epoch {}) +".format(epoch+1))

    train_acc = accuracy_score(train_true_rel, train_pred_rel)
    train_prec = precision_score(train_true_rel, train_pred_rel)
    train_rec = recall_score(train_true_rel, train_pred_rel)
    train_f1 = f1_score(train_true_rel, train_pred_rel)

    print(f"Relevance - Accuracy: {train_acc:.4f}, Precision: {train_prec:.4f}, Recall: {train_rec:.4f}, F1 Score: {train_f1:.4f}")

    relevant_train_mask = [bool(x) for x in train_true_rel]
    filtered_true_train_cat = [c for c, r in zip(train_true_cat, relevant_train_mask) if r == 1]
    filtered_pred_train_cat = [p for p, r in zip(train_pred_cat, relevant_train_mask) if r == 1]

    if filtered_true_train_cat:
        disaster_acc = accuracy_score(filtered_true_train_cat, filtered_pred_train_cat)
        disaster_f1 = f1_score(filtered_true_train_cat, filtered_pred_train_cat, average='weighted')
        print(f"Disaster Type - Accuracy: {disaster_acc:.4f}, F1 Score: {disaster_f1:.4f}")

    end_epoch_time = time.time()
    epoch_duration = end_epoch_time - start_epoch_time
    avg_epoch_loss = running_loss / train_steps

    if epoch_duration > 60:
        print(f"Epoch {epoch+1} finished in {epoch_duration/60:.2f} minutes")
    else:
        print(f"Epoch {epoch+1} finished in {epoch_duration:.2f} seconds")

    print(f"Average Training Loss: {avg_epoch_loss:.4f}")

    #Validation
    model.eval()
    val_loss = 0.0
    pred_rel, pred_cat = [], []
    true_rel, true_cat = [], []

    with torch.inference_mode():
        for batch in tqdm(val_loader, desc="Validation Progress", leave=False, dynamic_ncols=True):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            relevance = batch["relevance"].unsqueeze(1).to(device)
            disaster = batch["disaster"].to(device)

            out = model(input_ids=input_ids, attention_mask=attention_mask)

            loss = (
                F.binary_cross_entropy_with_logits(out["relevance"], relevance) +
                F.cross_entropy(out["disaster_type"], disaster)
            )
            val_loss += loss.item()

            logits = out["relevance"].squeeze()
            pred_rel.extend((logits > 0.5).int().cpu().tolist())
            pred_cat.extend(torch.argmax(out["disaster_type"], dim=1).cpu().tolist())
            true_rel.extend(relevance.cpu().int().tolist())
            true_cat.extend(disaster.cpu().tolist())

    avg_val_loss = val_loss / len(val_loader)

    # Validation metrics
    acc = accuracy_score(true_rel, pred_rel)
    prec = precision_score(true_rel, pred_rel)
    rec = recall_score(true_rel, pred_rel)
    f1 = f1_score(true_rel, pred_rel)

    print(f" Validation Results (Epoch {epoch+1}):")
    print(f"  - Validation Loss: {avg_val_loss:.4f}")
    print(f"  - Accuracy: {acc:.4f}")
    print(f"  - Precision: {prec:.4f}")
    print(f"  - Recall: {rec:.4f}")
    print(f"  - F1 Score: {f1:.4f}")

end_training_time = time.time()
total_time = end_training_time - start_training_time

print("\nTraining Finished")
if total_time > 60:
    print(f"Total Training Time: {total_time/60:.2f} minutes")
else:
    print(f"Total Training Time: {total_time:.2f} seconds")

# + Overall Relevance Metrics +
print("\n+ Relevance Metrics +")
acc_relevance = accuracy_score(true_rel, pred_rel)
prec_relevance = precision_score(true_rel, pred_rel)
rec_relevance = recall_score(true_rel, pred_rel)
f1_relevance = f1_score(true_rel, pred_rel)

print(f"Relevance Accuracy: {acc_relevance:.4f}")
print(f"Relevance Precision: {prec_relevance:.4f}")
print(f"Relevance Recall: {rec_relevance:.4f}")
print(f"Relevance F1 Score: {f1_relevance:.4f}")

# + Disaster Type Classification Metrics (only for Relevant posts) +
print("\n+ Disaster Type Classification Metrics (Relevant Only) +")
relevant_mask = [bool(x) for x in true_rel]  # Only look at posts that were "Relevant" = 1
filtered_true_cat = [c for c, r in zip(true_cat, relevant_mask) if r == 1]
filtered_pred_cat = [p for p, r in zip(pred_cat, relevant_mask) if r == 1]

acc_disaster = accuracy_score(filtered_true_cat, filtered_pred_cat)
prec_disaster = precision_score(filtered_true_cat, filtered_pred_cat, average='weighted')
rec_disaster = recall_score(filtered_true_cat, filtered_pred_cat, average='weighted')
f1_disaster = f1_score(filtered_true_cat, filtered_pred_cat, average='weighted')

print(f"Disaster Type Accuracy: {acc_disaster:.4f}")
print(f"Disaster Type Precision: {prec_disaster:.4f}")
print(f"Disaster Type Recall: {rec_disaster:.4f}")
print(f"Disaster Type F1 Score: {f1_disaster:.4f}")

# + Detailed Classification Report (Disaster Types) +
print("\n+ Detailed Disaster Type Report +")
print(classification_report(filtered_true_cat, filtered_pred_cat, target_names=category_map))


torch.save(model.state_dict(), "multitask_bertweet_model.pth")
print("Model saved to multitask_bertweet_model.pth")

def predict_from_csv(model_path, input_csv_path, output_csv_path):
    print("Loading model...")
    model = MultiTaskDisasterModel().to(device)
    model.load_state_dict(torch.load(model_path))
    model.eval()

    print("Loading test data...")

    # Load CSV
    test_df = pd.read_csv(input_csv_path)
    test_df.columns = test_df.columns.str.strip()

    if 'Tweet Text' in test_df.columns:
        test_df = test_df.rename(columns={"Tweet Text": "text"})
    elif 'post_text' in test_df.columns:
        test_df = test_df.rename(columns={"post_text": "text"})
    elif 'post' in test_df.columns:
        test_df = test_df.rename(columns={"post": "text"})

    if 'text' not in test_df.columns:
        print("Available columns are:", list(test_df.columns))
        raise ValueError("No 'text' column found.")

    print(f"Loaded {len(test_df)} posts.")

    # preprocess
    test_df['cleaned_text'] = test_df['text'].apply(preprocess_data)

    # drop empty or bad text rows
    test_df['text'] = test_df['text'].fillna("")          # fill NaN with ""
    test_df = test_df[test_df['text'].str.strip() != ""]   # dropping rows that are empty after strip()

    print(f"{len(test_df)} posts ready after cleaning.")


    # Tokenize
    test_encodings = tokenizer(
        test_df['text'].tolist(),
        truncation=True,
        padding="max_length",
        max_length=128
    )

    class SimpleTestDataset(Dataset):
        def __init__(self, encodings):
            self.input_ids = encodings['input_ids']
            self.attention_mask = encodings['attention_mask']

        def __len__(self):
            return len(self.input_ids)

        def __getitem__(self, idx):
            return {
                "input_ids": torch.tensor(self.input_ids[idx]),
                "attention_mask": torch.tensor(self.attention_mask[idx])
            }

    test_dataset = SimpleTestDataset(test_encodings)
    test_loader = DataLoader(test_dataset, batch_size=64)

    print("Running predictions...")

    all_relevancy_preds = []
    all_disaster_preds = []

    with torch.inference_mode():
        for batch in tqdm(test_loader, desc="Predicting", dynamic_ncols=True):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)

            relevancy_logits = outputs['relevance'].squeeze()
            relevancy_preds = (relevancy_logits > 0.5).int().cpu().tolist()
            all_relevancy_preds.extend(relevancy_preds)

            disaster_logits = outputs['disaster_type']
            disaster_preds = torch.argmax(disaster_logits, dim=1).cpu().tolist()
            all_disaster_preds.extend(disaster_preds)

    idx_to_cat = {i: c for i, c in enumerate(category_map)}
    test_df['Predicted_Relevancy'] = all_relevancy_preds
    test_df['Predicted_Disaster_Label'] = all_disaster_preds
    test_df['Predicted_Disaster_Category'] = test_df['Predicted_Disaster_Label'].map(idx_to_cat)

    test_df.to_csv(output_csv_path, index=False)
    print(f"Saved predictions to {output_csv_path}")

    return test_df

predict_from_csv(
    model_path="multitask_bertweet_model.pth",
    input_csv_path="/content/KenBLUESKYPOSTSPART2.csv",
    output_csv_path="/content/KenBLUESKYPOSTSPART2_Predictions.csv"
)

