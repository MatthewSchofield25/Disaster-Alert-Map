{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewSchofield25/Weather-Emergency-Application/blob/main/CS4485_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "#from vaderSentiment.vaderSentiment\n",
        "# JUST NOTES NOT CODE\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "sentence = \"I LOVE waking up in the middle of night. Yes. Totally.\"\n",
        "sid_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # polarity_scores method of SentimentIntensityAnalyzer object gives a sentiment dictionary.\n",
        "    # which contains pos, neg, neu, and compound scores.\n",
        "sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "print(sentiment_dict['compound'])\n",
        "print(\"Sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
        "print(\"Sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
        "print(\"Sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "NBvl44pgzzGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3294e183-b265-4c6d-d5ac-46d49e01fde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.824\n",
            "Sentence was rated as  0.0 % Negative\n",
            "Sentence was rated as  51.2 % Neutral\n",
            "Sentence was rated as  48.8 % Positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7kVcB15LtE3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97edfecb-5070-407f-c28c-0752b8fe1c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('fire', 356), ('new', 228), ('news', 213), ('people', 201), ('time', 181), ('year', 178), ('video', 175), ('disaster', 162), ('emergency', 159), ('body', 155), ('day', 151), ('home', 144), ('police', 143), ('building', 141), ('life', 132), ('family', 132), ('storm', 128), ('crash', 125), ('california', 121), ('burning', 121)]\n",
            "[('suicide bomber', 60), ('burning building', 59), ('body bag', 51), ('youtube video', 43), ('liked youtube', 42), ('northern california', 41), ('cross body', 40), ('oil spill', 39), ('suicide bombing', 36), ('california wildfire', 35), ('year old', 35), ('mass murder', 33), ('heat wave', 31), ('full read', 31), ('natural disaster', 31), ('mass murderer', 31), ('forest fire', 30), ('prebreak best', 30), ('bomber detonated', 30), ('home razed', 29)]\n",
            "[('liked youtube video', 42), ('suicide bomber detonated', 30), ('northern california wildfire', 29), ('latest home razed', 28), ('home razed northern', 28), ('pkk suicide bomber', 28), ('bomber detonated bomb', 28), ('razed northern california', 27), ('old pkk suicide', 27), ('family sue legionnaire', 26), ('family affected fatal', 26), ('affected fatal outbreak', 26), ('cross body bag', 25), ('obama declares disaster', 25), ('declares disaster typhoon', 25), ('disaster typhoon devastated', 25), ('typhoon devastated saipan', 25), ('sue legionnaire family', 25), ('legionnaire family affected', 25), ('wreckage conclusively confirmed', 25)]\n"
          ]
        }
      ],
      "source": [
        "# Main Steps:\n",
        "# Data Preprocessing: Tokenize data and remove punctuation. Don't forget to .lower\n",
        "# NLP: Process Data; Detect frequency of words, n-grams (BOW), TF-IDF, possibly Glove Vectors.\n",
        "# Model: We can use different models and test their accuracies. LTSM possibly\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Used for ML/Deep Learning Algorithms\n",
        "from tensorflow.keras.layers import Embedding,Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM,Bidirectional,GRU,MaxPooling1D,Conv1D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "#from catboost import CatBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Sentiment analysis\n",
        "nltk.download(\"vader_lexicon\")\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Labels on the data\n",
        "#id : A unique identifier for each tweet.\n",
        "#keyword : A particular keyword from the tweet (may be blank).\n",
        "#location: The location the tweet was sent from (may be blank).\n",
        "#text : The text of the tweet.\n",
        "#target : This denotes whether a tweet is about a real disaster (1) or not (0).\n",
        "\n",
        "common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest'\n",
        "                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']\n",
        "\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "X = train.drop(columns=[\"target\"],axis=1)\n",
        "y = train[\"target\"]\n",
        "\n",
        "# Load the SpaCy NLP model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize the GeoPy geocoder\n",
        "geolocator = Nominatim(user_agent='project_app')\n",
        "\n",
        "def text_cleaning(data):\n",
        "    return ' '.join(i for i in data.split() if i not in common_words)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    '''\n",
        "    Input: Data to be cleaned.\n",
        "    Output: Cleaned Data.\n",
        "\n",
        "    '''\n",
        "    review =re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ',data) #removal of url\n",
        "    review =re.sub(r'<.*?>',' ',review) #removal of html tags\n",
        "    review = re.sub(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\",' ',review)\n",
        "    review = re.sub('[^a-zA-Z]',' ',review) # filtering out miscellaneous text.\n",
        "    review = review.lower() # Lowering all the words in text\n",
        "    review = review.split() # split into a list of words\n",
        "    review = [lm.lemmatize(words) for words in review if words not in stopwords.words('english')] # Turn words into their stems/roots\n",
        "    review = [i for i in review if len(i)>2] # Removal of words with length<2\n",
        "    review = ' '.join(review) # Put back to single string with a space separator\n",
        "    return review\n",
        "\n",
        "def sentiment_ana(data):\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        "    sentiment_dict = sid_obj.polarity_scores(data)\n",
        "    return sentiment_dict['compound']\n",
        "\n",
        "def sentiment_ana_label(data):\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        "    sentiment_dict = sid_obj.polarity_scores(data)\n",
        "    compound_score = sentiment_dict['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Define a function to extract location names from a text using SpaCy NER\n",
        "def extract_locations(text):\n",
        "    doc = nlp(text)\n",
        "    #print(\"Entities:\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    return [ent.text for ent in doc.ents if ent.label_ in ['LOC', 'GPE']]\n",
        "\n",
        "# Define a function to map location names to countries using GeoPy geocoding\n",
        "def map_locations_to_countries(locations):\n",
        "    countries = set()\n",
        "    for location in locations:\n",
        "        try:\n",
        "            location = geolocator.geocode(location, addressdetails=True, exactly_one=True, timeout=10)\n",
        "            country = location.raw['address']['country']\n",
        "            countries.add(country)\n",
        "        except:\n",
        "            pass\n",
        "    return countries\n",
        "\n",
        "\n",
        "def top_ngrams(data,n,grams):\n",
        "\n",
        "    if grams == 1:\n",
        "        count_vec = CountVectorizer(ngram_range=(1,1)).fit(data)\n",
        "        bow = count_vec.transform(data)\n",
        "        add_words = bow.sum(axis=0)\n",
        "        word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n",
        "        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
        "    elif grams == 2:\n",
        "        count_vec = CountVectorizer(ngram_range=(2,2)).fit(data)\n",
        "        bow = count_vec.transform(data)\n",
        "        add_words = bow.sum(axis=0)\n",
        "        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]\n",
        "        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
        "    elif grams == 3:\n",
        "        count_vec = CountVectorizer(ngram_range=(3,3)).fit(data)\n",
        "        bow = count_vec.transform(data)\n",
        "        add_words = bow.sum(axis=0)\n",
        "        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]\n",
        "        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "    return word_freq[:n]\n",
        "\n",
        "train[\"Cleaned_text\"] = train[\"text\"].apply(preprocess_data)\n",
        "test[\"Cleaned_text\"] = test[\"text\"].apply(preprocess_data)\n",
        "\n",
        "# Find common words and get rid of words that are unneeded\n",
        "train[\"Cleaned_text\"] = train[\"Cleaned_text\"].apply(text_cleaning)\n",
        "test[\"Cleaned_text\"] = test[\"Cleaned_text\"].apply(text_cleaning)\n",
        "\n",
        "train[\"Sentiment\"] = train[\"text\"].apply(sentiment_ana)\n",
        "test[\"Sentiment\"] = test[\"text\"].apply(sentiment_ana)\n",
        "\n",
        "train[\"Sentiment_Label\"] = train[\"text\"].apply(sentiment_ana_label)\n",
        "test[\"Sentiment_Label\"] = test[\"text\"].apply(sentiment_ana_label)\n",
        "\n",
        "train[\"Location_Test\"] = train[\"text\"].apply(extract_locations)\n",
        "#train[\"Country_Test\"] = train[\"Location_Test\"].apply(map_locations_to_countries)\n",
        "\n",
        "\n",
        "\n",
        "common_words_uni = top_ngrams(train[\"Cleaned_text\"],20,1)\n",
        "common_words_bi = top_ngrams(train[\"Cleaned_text\"],20,2)\n",
        "common_words_tri = top_ngrams(train[\"Cleaned_text\"],20,3)\n",
        "\n",
        "print(common_words_uni)\n",
        "print(common_words_bi)\n",
        "print(common_words_tri)\n",
        "\n",
        "\n",
        "#train.head(50)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZHbQgFybGWTF",
        "outputId": "7cf18b7c-58eb-48e3-8f32-648a09efd943"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    id keyword                       location  \\\n",
              "0    1     NaN                            NaN   \n",
              "1    4     NaN                            NaN   \n",
              "2    5     NaN                            NaN   \n",
              "3    6     NaN                            NaN   \n",
              "4    7     NaN                            NaN   \n",
              "5    8     NaN                            NaN   \n",
              "6   10     NaN                            NaN   \n",
              "7   13     NaN                            NaN   \n",
              "8   14     NaN                            NaN   \n",
              "9   15     NaN                            NaN   \n",
              "10  16     NaN                            NaN   \n",
              "11  17     NaN                            NaN   \n",
              "12  18     NaN                            NaN   \n",
              "13  19     NaN                            NaN   \n",
              "14  20     NaN                            NaN   \n",
              "15  23     NaN                            NaN   \n",
              "16  24     NaN                            NaN   \n",
              "17  25     NaN                            NaN   \n",
              "18  26     NaN                            NaN   \n",
              "19  28     NaN                            NaN   \n",
              "20  31     NaN                            NaN   \n",
              "21  32     NaN                            NaN   \n",
              "22  33     NaN                            NaN   \n",
              "23  34     NaN                            NaN   \n",
              "24  36     NaN                            NaN   \n",
              "25  37     NaN                            NaN   \n",
              "26  38     NaN                            NaN   \n",
              "27  39     NaN                            NaN   \n",
              "28  40     NaN                            NaN   \n",
              "29  41     NaN                            NaN   \n",
              "30  44     NaN                            NaN   \n",
              "31  48  ablaze                     Birmingham   \n",
              "32  49  ablaze  Est. September 2012 - Bristol   \n",
              "33  50  ablaze                         AFRICA   \n",
              "34  52  ablaze               Philadelphia, PA   \n",
              "35  53  ablaze                     London, UK   \n",
              "36  54  ablaze                       Pretoria   \n",
              "37  55  ablaze                   World Wide!!   \n",
              "38  56  ablaze                            NaN   \n",
              "39  57  ablaze                 Paranaque City   \n",
              "40  59  ablaze                 Live On Webcam   \n",
              "41  61  ablaze                            NaN   \n",
              "42  62  ablaze                      milky way   \n",
              "43  63  ablaze                            NaN   \n",
              "44  64  ablaze                            NaN   \n",
              "45  65  ablaze                            NaN   \n",
              "46  66  ablaze      GREENSBORO,NORTH CAROLINA   \n",
              "47  67  ablaze                            NaN   \n",
              "48  68  ablaze                 Live On Webcam   \n",
              "49  71  ablaze                       England.   \n",
              "\n",
              "                                                 text  target  \\\n",
              "0   Our Deeds are the Reason of this #earthquake M...       1   \n",
              "1              Forest fire near La Ronge Sask. Canada       1   \n",
              "2   All residents asked to 'shelter in place' are ...       1   \n",
              "3   13,000 people receive #wildfires evacuation or...       1   \n",
              "4   Just got sent this photo from Ruby #Alaska as ...       1   \n",
              "5   #RockyFire Update => California Hwy. 20 closed...       1   \n",
              "6   #flood #disaster Heavy rain causes flash flood...       1   \n",
              "7   I'm on top of the hill and I can see a fire in...       1   \n",
              "8   There's an emergency evacuation happening now ...       1   \n",
              "9   I'm afraid that the tornado is coming to our a...       1   \n",
              "10        Three people died from the heat wave so far       1   \n",
              "11  Haha South Tampa is getting flooded hah- WAIT ...       1   \n",
              "12  #raining #flooding #Florida #TampaBay #Tampa 1...       1   \n",
              "13            #Flood in Bago Myanmar #We arrived Bago       1   \n",
              "14  Damage to school bus on 80 in multi car crash ...       1   \n",
              "15                                     What's up man?       0   \n",
              "16                                      I love fruits       0   \n",
              "17                                   Summer is lovely       0   \n",
              "18                                  My car is so fast       0   \n",
              "19                       What a goooooooaaaaaal!!!!!!       0   \n",
              "20                             this is ridiculous....       0   \n",
              "21                                  London is cool ;)       0   \n",
              "22                                        Love skiing       0   \n",
              "23                              What a wonderful day!       0   \n",
              "24                                           LOOOOOOL       0   \n",
              "25                     No way...I can't eat that shit       0   \n",
              "26                              Was in NYC last week!       0   \n",
              "27                                 Love my girlfriend       0   \n",
              "28                                          Cooool :)       0   \n",
              "29                                 Do you like pasta?       0   \n",
              "30                                           The end!       0   \n",
              "31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   \n",
              "32  We always try to bring the heavy. #metal #RT h...       0   \n",
              "33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   \n",
              "34                 Crying out for more! Set me ablaze       0   \n",
              "35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0   \n",
              "36  @PhDSquares #mufc they've built so much hype a...       0   \n",
              "37  INEC Office in Abia Set Ablaze - http://t.co/3...       1   \n",
              "38  Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...       1   \n",
              "39                             Ablaze for you Lord :D       0   \n",
              "40  Check these out: http://t.co/rOI2NSmEJJ http:/...       0   \n",
              "41  on the outside you're ablaze and alive\\nbut yo...       0   \n",
              "42  Had an awesome time visiting the CFC head offi...       0   \n",
              "43       SOOOO PUMPED FOR ABLAZE ???? @southridgelife       0   \n",
              "44  I wanted to set Chicago ablaze with my preachi...       0   \n",
              "45  I gained 3 followers in the last week. You? Kn...       0   \n",
              "46  How the West was burned: Thousands of wildfire...       1   \n",
              "47  Building the perfect tracklist to life leave t...       0   \n",
              "48  Check these out: http://t.co/rOI2NSmEJJ http:/...       0   \n",
              "49  First night with retainers in. It's quite weir...       0   \n",
              "\n",
              "                                         Cleaned_text  Sentiment  \\\n",
              "0                deed reason earthquake allah forgive     0.2732   \n",
              "1                  forest fire near ronge sask canada    -0.3400   \n",
              "2   resident asked shelter place notified officer ...    -0.2960   \n",
              "3   people receive wildfire evacuation order calif...     0.0000   \n",
              "4   sent photo ruby alaska smoke wildfire pours sc...     0.0000   \n",
              "5   rockyfire update california hwy closed directi...    -0.3400   \n",
              "6   flood disaster heavy rain cause flash flooding...     0.0000   \n",
              "7                                  top hill fire wood    -0.1531   \n",
              "8   emergency evacuation happening building across...    -0.3818   \n",
              "9                          afraid tornado coming area     0.0000   \n",
              "10                    three people died heat wave far    -0.5574   \n",
              "11  haha south tampa getting flooded hah wait seco...     0.4588   \n",
              "12  raining flooding florida tampabay tampa day lo...    -0.3182   \n",
              "13                    flood bago myanmar arrived bago     0.0000   \n",
              "14         damage school bus multi car crash breaking    -0.7096   \n",
              "15                                                man     0.0000   \n",
              "16                                         love fruit     0.6369   \n",
              "17                                      summer lovely     0.5859   \n",
              "18                                           car fast     0.0000   \n",
              "19                                    goooooooaaaaaal     0.0000   \n",
              "20                                         ridiculous     0.0000   \n",
              "21                                        london cool     0.4939   \n",
              "22                                        love skiing     0.6369   \n",
              "23                                      wonderful day     0.6114   \n",
              "24                                           looooool     0.0000   \n",
              "25                                           eat shit     0.1838   \n",
              "26                                      nyc last week     0.0000   \n",
              "27                                    love girlfriend     0.6369   \n",
              "28                                             cooool     0.4588   \n",
              "29                                              pasta     0.3612   \n",
              "30                                                end     0.0000   \n",
              "31                     bbcmtd wholesale market ablaze     0.0000   \n",
              "32                       always try bring heavy metal     0.0000   \n",
              "33  africanbaze breaking news nigeria flag set abl...     0.0000   \n",
              "34                                     cry set ablaze    -0.5255   \n",
              "35                    plus side sky last night ablaze     0.0000   \n",
              "36  phdsquares mufc built much hype around new acq...    -0.5023   \n",
              "37                        inec office abia set ablaze     0.0000   \n",
              "38  barbados bridgetown jamaica car set ablaze san...     0.0000   \n",
              "39                                        ablaze lord     0.6166   \n",
              "40                                         check nsfw     0.0000   \n",
              "41                   outside ablaze alive dead inside    -0.7311   \n",
              "42  awesome time visiting cfc head office ancop si...     0.8900   \n",
              "43                 soooo pumped ablaze southridgelife     0.0000   \n",
              "44          wanted set chicago ablaze preaching hotel     0.0000   \n",
              "45               gained follower last week stats grow     0.3818   \n",
              "46  west burned thousand wildfire ablaze californi...    -0.2500   \n",
              "47  building perfect tracklist life leave street a...     0.5423   \n",
              "48                                         check nsfw     0.0000   \n",
              "49  night retainer quite weird better used wear ev...     0.2927   \n",
              "\n",
              "   Sentiment_Label                Location_Test  \n",
              "0         positive                           []  \n",
              "1         negative                     [Canada]  \n",
              "2         negative                           []  \n",
              "3          neutral                 [California]  \n",
              "4          neutral               [Ruby, Alaska]  \n",
              "5         negative                [Lake County]  \n",
              "6          neutral  [Manitou, Colorado Springs]  \n",
              "7         negative                           []  \n",
              "8         negative                           []  \n",
              "9          neutral                           []  \n",
              "10        negative                           []  \n",
              "11        positive                [South Tampa]  \n",
              "12        negative                      [Tampa]  \n",
              "13         neutral                    [Myanmar]  \n",
              "14        negative                           []  \n",
              "15         neutral                           []  \n",
              "16        positive                           []  \n",
              "17        positive                           []  \n",
              "18         neutral                           []  \n",
              "19         neutral                           []  \n",
              "20         neutral                           []  \n",
              "21        positive                     [London]  \n",
              "22        positive                           []  \n",
              "23        positive                           []  \n",
              "24         neutral                           []  \n",
              "25        positive                           []  \n",
              "26         neutral                        [NYC]  \n",
              "27        positive                           []  \n",
              "28        positive                           []  \n",
              "29        positive                           []  \n",
              "30         neutral                           []  \n",
              "31         neutral                           []  \n",
              "32         neutral                           []  \n",
              "33         neutral                    [Nigeria]  \n",
              "34        negative                           []  \n",
              "35         neutral                           []  \n",
              "36        negative                           []  \n",
              "37         neutral                       [Abia]  \n",
              "38         neutral                           []  \n",
              "39        positive                           []  \n",
              "40         neutral                           []  \n",
              "41        negative                           []  \n",
              "42        positive                           []  \n",
              "43         neutral                           []  \n",
              "44         neutral                    [Chicago]  \n",
              "45        positive                           []  \n",
              "46        negative           [West, California]  \n",
              "47        positive                           []  \n",
              "48         neutral                           []  \n",
              "49        positive                           []  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7932cb9e-1034-421c-8cbf-8db53f3d037b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>Cleaned_text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentiment_Label</th>\n",
              "      <th>Location_Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>deed reason earthquake allah forgive</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>forest fire near ronge sask canada</td>\n",
              "      <td>-0.3400</td>\n",
              "      <td>negative</td>\n",
              "      <td>[Canada]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "      <td>-0.2960</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>people receive wildfire evacuation order calif...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[California]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>sent photo ruby alaska smoke wildfire pours sc...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[Ruby, Alaska]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
              "      <td>1</td>\n",
              "      <td>rockyfire update california hwy closed directi...</td>\n",
              "      <td>-0.3400</td>\n",
              "      <td>negative</td>\n",
              "      <td>[Lake County]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
              "      <td>1</td>\n",
              "      <td>flood disaster heavy rain cause flash flooding...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[Manitou, Colorado Springs]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
              "      <td>1</td>\n",
              "      <td>top hill fire wood</td>\n",
              "      <td>-0.1531</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There's an emergency evacuation happening now ...</td>\n",
              "      <td>1</td>\n",
              "      <td>emergency evacuation happening building across...</td>\n",
              "      <td>-0.3818</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
              "      <td>1</td>\n",
              "      <td>afraid tornado coming area</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Three people died from the heat wave so far</td>\n",
              "      <td>1</td>\n",
              "      <td>three people died heat wave far</td>\n",
              "      <td>-0.5574</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
              "      <td>1</td>\n",
              "      <td>haha south tampa getting flooded hah wait seco...</td>\n",
              "      <td>0.4588</td>\n",
              "      <td>positive</td>\n",
              "      <td>[South Tampa]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
              "      <td>1</td>\n",
              "      <td>raining flooding florida tampabay tampa day lo...</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>negative</td>\n",
              "      <td>[Tampa]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
              "      <td>1</td>\n",
              "      <td>flood bago myanmar arrived bago</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[Myanmar]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
              "      <td>1</td>\n",
              "      <td>damage school bus multi car crash breaking</td>\n",
              "      <td>-0.7096</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>23</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What's up man?</td>\n",
              "      <td>0</td>\n",
              "      <td>man</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>24</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I love fruits</td>\n",
              "      <td>0</td>\n",
              "      <td>love fruit</td>\n",
              "      <td>0.6369</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Summer is lovely</td>\n",
              "      <td>0</td>\n",
              "      <td>summer lovely</td>\n",
              "      <td>0.5859</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My car is so fast</td>\n",
              "      <td>0</td>\n",
              "      <td>car fast</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>28</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
              "      <td>0</td>\n",
              "      <td>goooooooaaaaaal</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>this is ridiculous....</td>\n",
              "      <td>0</td>\n",
              "      <td>ridiculous</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>32</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>London is cool ;)</td>\n",
              "      <td>0</td>\n",
              "      <td>london cool</td>\n",
              "      <td>0.4939</td>\n",
              "      <td>positive</td>\n",
              "      <td>[London]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Love skiing</td>\n",
              "      <td>0</td>\n",
              "      <td>love skiing</td>\n",
              "      <td>0.6369</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What a wonderful day!</td>\n",
              "      <td>0</td>\n",
              "      <td>wonderful day</td>\n",
              "      <td>0.6114</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>36</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LOOOOOOL</td>\n",
              "      <td>0</td>\n",
              "      <td>looooool</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>37</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No way...I can't eat that shit</td>\n",
              "      <td>0</td>\n",
              "      <td>eat shit</td>\n",
              "      <td>0.1838</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Was in NYC last week!</td>\n",
              "      <td>0</td>\n",
              "      <td>nyc last week</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[NYC]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>39</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Love my girlfriend</td>\n",
              "      <td>0</td>\n",
              "      <td>love girlfriend</td>\n",
              "      <td>0.6369</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cooool :)</td>\n",
              "      <td>0</td>\n",
              "      <td>cooool</td>\n",
              "      <td>0.4588</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>41</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Do you like pasta?</td>\n",
              "      <td>0</td>\n",
              "      <td>pasta</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>44</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The end!</td>\n",
              "      <td>0</td>\n",
              "      <td>end</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>48</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Birmingham</td>\n",
              "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
              "      <td>1</td>\n",
              "      <td>bbcmtd wholesale market ablaze</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>49</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Est. September 2012 - Bristol</td>\n",
              "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
              "      <td>0</td>\n",
              "      <td>always try bring heavy metal</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>50</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>AFRICA</td>\n",
              "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
              "      <td>1</td>\n",
              "      <td>africanbaze breaking news nigeria flag set abl...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[Nigeria]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>52</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Philadelphia, PA</td>\n",
              "      <td>Crying out for more! Set me ablaze</td>\n",
              "      <td>0</td>\n",
              "      <td>cry set ablaze</td>\n",
              "      <td>-0.5255</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>53</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>London, UK</td>\n",
              "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
              "      <td>0</td>\n",
              "      <td>plus side sky last night ablaze</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>54</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Pretoria</td>\n",
              "      <td>@PhDSquares #mufc they've built so much hype a...</td>\n",
              "      <td>0</td>\n",
              "      <td>phdsquares mufc built much hype around new acq...</td>\n",
              "      <td>-0.5023</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>55</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>World Wide!!</td>\n",
              "      <td>INEC Office in Abia Set Ablaze - http://t.co/3...</td>\n",
              "      <td>1</td>\n",
              "      <td>inec office abia set ablaze</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[Abia]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>56</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...</td>\n",
              "      <td>1</td>\n",
              "      <td>barbados bridgetown jamaica car set ablaze san...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>57</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Paranaque City</td>\n",
              "      <td>Ablaze for you Lord :D</td>\n",
              "      <td>0</td>\n",
              "      <td>ablaze lord</td>\n",
              "      <td>0.6166</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>59</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Live On Webcam</td>\n",
              "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
              "      <td>0</td>\n",
              "      <td>check nsfw</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>61</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>NaN</td>\n",
              "      <td>on the outside you're ablaze and alive\\nbut yo...</td>\n",
              "      <td>0</td>\n",
              "      <td>outside ablaze alive dead inside</td>\n",
              "      <td>-0.7311</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>62</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>milky way</td>\n",
              "      <td>Had an awesome time visiting the CFC head offi...</td>\n",
              "      <td>0</td>\n",
              "      <td>awesome time visiting cfc head office ancop si...</td>\n",
              "      <td>0.8900</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>63</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SOOOO PUMPED FOR ABLAZE ???? @southridgelife</td>\n",
              "      <td>0</td>\n",
              "      <td>soooo pumped ablaze southridgelife</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>64</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I wanted to set Chicago ablaze with my preachi...</td>\n",
              "      <td>0</td>\n",
              "      <td>wanted set chicago ablaze preaching hotel</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[Chicago]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>65</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I gained 3 followers in the last week. You? Kn...</td>\n",
              "      <td>0</td>\n",
              "      <td>gained follower last week stats grow</td>\n",
              "      <td>0.3818</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>66</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>GREENSBORO,NORTH CAROLINA</td>\n",
              "      <td>How the West was burned: Thousands of wildfire...</td>\n",
              "      <td>1</td>\n",
              "      <td>west burned thousand wildfire ablaze californi...</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>negative</td>\n",
              "      <td>[West, California]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>67</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Building the perfect tracklist to life leave t...</td>\n",
              "      <td>0</td>\n",
              "      <td>building perfect tracklist life leave street a...</td>\n",
              "      <td>0.5423</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>68</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Live On Webcam</td>\n",
              "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
              "      <td>0</td>\n",
              "      <td>check nsfw</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>71</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>England.</td>\n",
              "      <td>First night with retainers in. It's quite weir...</td>\n",
              "      <td>0</td>\n",
              "      <td>night retainer quite weird better used wear ev...</td>\n",
              "      <td>0.2927</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7932cb9e-1034-421c-8cbf-8db53f3d037b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7932cb9e-1034-421c-8cbf-8db53f3d037b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7932cb9e-1034-421c-8cbf-8db53f3d037b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b7fa8adb-35a7-447d-b294-ef12ebb9fc12\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7fa8adb-35a7-447d-b294-ef12ebb9fc12')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b7fa8adb-35a7-447d-b294-ef12ebb9fc12 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 7613,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3137,\n        \"min\": 1,\n        \"max\": 10873,\n        \"num_unique_values\": 7613,\n        \"samples\": [\n          3796,\n          3185,\n          7769\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 221,\n        \"samples\": [\n          \"injury\",\n          \"nuclear%20reactor\",\n          \"engulfed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3341,\n        \"samples\": [\n          \"Oklahoma\",\n          \"Starling City\",\n          \"Trinidad and Tobago\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7503,\n        \"samples\": [\n          \"Three Homes Demolished in Unrecognized Arab Village - International Middle East Media Center http://t.co/ik8m4Yi9T4\",\n          \"Reid Lake fire prompts campground evacuation order http://t.co/jBODKM6rBU\",\n          \"FAAN orders evacuation of abandoned aircraft at MMA http://t.co/dEvYbnVXGQ via @todayng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cleaned_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6803,\n        \"samples\": [\n          \"least dead million displaced india flood india reliefweb\",\n          \"evahanderek marleyknysh time bus driver held hostage mall parking lot lmfao\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4569557196874852,\n        \"min\": -0.9883,\n        \"max\": 0.973,\n        \"num_unique_values\": 1287,\n        \"samples\": [\n          -0.5754,\n          0.9072\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Location_Test\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def top_ngrams(data,n,grams):\n",
        "\n",
        "    if grams == 1:\n",
        "        count_vec = CountVectorizer(ngram_range=(1,1)).fit(data)\n",
        "        bow = count_vec.transform(data)\n",
        "        add_words = bow.sum(axis=0)\n",
        "        word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n",
        "        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
        "    elif grams == 2:\n",
        "        count_vec = CountVectorizer(ngram_range=(2,2)).fit(data)\n",
        "        bow = count_vec.transform(data)\n",
        "        add_words = bow.sum(axis=0)\n",
        "        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]\n",
        "        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
        "    elif grams == 3:\n",
        "        count_vec = CountVectorizer(ngram_range=(3,3)).fit(data)\n",
        "        bow = count_vec.transform(data)\n",
        "        add_words = bow.sum(axis=0)\n",
        "        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]\n",
        "        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "    return word_freq[:n]\n",
        "\n",
        "common_words_uni = top_ngrams(train[\"Cleaned_text\"],20,1)\n",
        "common_words_bi = top_ngrams(train[\"Cleaned_text\"],20,2)\n",
        "common_words_tri = top_ngrams(train[\"Cleaned_text\"],20,3)\n",
        "\n",
        "print(common_words_uni)\n",
        "print(common_words_bi)\n",
        "print(common_words_tri)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxsM6UyPw1rP",
        "outputId": "1874552a-246d-4661-834a-5b446d94d75c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('fire', 356), ('new', 228), ('news', 213), ('people', 201), ('time', 181), ('year', 178), ('video', 175), ('disaster', 162), ('emergency', 159), ('body', 155), ('day', 151), ('home', 144), ('police', 143), ('building', 141), ('life', 132), ('family', 132), ('storm', 128), ('crash', 125), ('california', 121), ('burning', 121)]\n",
            "[('suicide bomber', 60), ('burning building', 59), ('body bag', 51), ('youtube video', 43), ('liked youtube', 42), ('northern california', 41), ('cross body', 40), ('oil spill', 39), ('suicide bombing', 36), ('california wildfire', 35), ('year old', 35), ('mass murder', 33), ('heat wave', 31), ('full read', 31), ('natural disaster', 31), ('mass murderer', 31), ('forest fire', 30), ('prebreak best', 30), ('bomber detonated', 30), ('home razed', 29)]\n",
            "[('liked youtube video', 42), ('suicide bomber detonated', 30), ('northern california wildfire', 29), ('latest home razed', 28), ('home razed northern', 28), ('pkk suicide bomber', 28), ('bomber detonated bomb', 28), ('razed northern california', 27), ('old pkk suicide', 27), ('family sue legionnaire', 26), ('family affected fatal', 26), ('affected fatal outbreak', 26), ('cross body bag', 25), ('obama declares disaster', 25), ('declares disaster typhoon', 25), ('disaster typhoon devastated', 25), ('typhoon devastated saipan', 25), ('sue legionnaire family', 25), ('legionnaire family affected', 25), ('wreckage conclusively confirmed', 25)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "nltk.download('punkt_tab')\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Function to preprocess and tokenize the text for Word2Vec\n",
        "def tokenize_data(data):\n",
        "    # Tokenize each cleaned text entry\n",
        "    return [word_tokenize(review) for review in data]\n",
        "\n",
        "# Tokenize the cleaned text in train and test datasets\n",
        "train_tokens = tokenize_data(train[\"Cleaned_text\"])\n",
        "test_tokens = tokenize_data(test[\"Cleaned_text\"])\n",
        "\n",
        "# Train a Word2Vec model on the tokenized training data\n",
        "word2vec_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "# Save the model for later use\n",
        "word2vec_model.save(\"word2vec_model.model\")\n",
        "\n",
        "\n",
        "# Function to get the average word vector for a sentence\n",
        "def get_sentence_embedding(sentence, model):\n",
        "    word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(model.vector_size)  # Return a zero vector if no words in the model\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Generate sentence embeddings for the train and test sets\n",
        "train_embeddings = [get_sentence_embedding(review, word2vec_model) for review in train_tokens]\n",
        "test_embeddings = [get_sentence_embedding(review, word2vec_model) for review in test_tokens]\n",
        "\n",
        "\n",
        "# Pad the sequences to ensure uniform length\n",
        "max_sequence_length = max([len(seq) for seq in train_embeddings])  # Max length in the training set\n",
        "\n",
        "\n",
        "train_embeddings_padded = pad_sequences(train_embeddings, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "test_embeddings_padded = pad_sequences(test_embeddings, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "\n",
        "embedding_feature_vector = 200 # Since we used glove vector embedding of dim 200.\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(max_sequence_length, word2vec_model.vector_size), return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Binary classification (for example)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Get predictions on the test data\n",
        "#predictions_prob = model.predict(test_embeddings_padded)  # Probability values between 0 and 1\n",
        "\n",
        "# Convert probabilities to class labels (0 or 1)\n",
        "#predictions = (predictions_prob > 0.5).astype(int)\n",
        "\n",
        "# Show the first few predictions\n",
        "#print(predictions[:10])  # Display the first 10 predictions\n",
        "\n",
        "# If you want to evaluate the accuracy (if you have the true labels in test):\n",
        "# If you have the true labels for the test set, you can compare the predictions with them:\n",
        "# test_labels = ...  # Ensure you have the true labels for the test set\n",
        "# accuracy = accuracy_score(test_labels, predictions)\n",
        "# print(f'Accuracy on Test Data: {accuracy}')\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Ymiex5rizXPH",
        "outputId": "a5f5f607-c3b1-4364-8a9f-00f6e4d28578"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m125,569\u001b[0m (490.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,569</span> (490.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m125,569\u001b[0m (490.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,569</span> (490.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "import spacy\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "# Load the SpaCy NLP model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize the GeoPy geocoder\n",
        "geolocator = Nominatim(user_agent='project_app')\n",
        "\n",
        "# Define a function to extract location names from a text using SpaCy NER\n",
        "def extract_locations(text):\n",
        "    doc = nlp(text)\n",
        "    print(\"Entities:\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    return [ent.text for ent in doc.ents if ent.label_ in ['LOC', 'GPE']]\n",
        "\n",
        "# Define a function to map location names to countries using GeoPy geocoding\n",
        "def map_locations_to_countries(locations):\n",
        "    countries = set()\n",
        "    for location in locations:\n",
        "        try:\n",
        "            location = geolocator.geocode(location, addressdetails=True, exactly_one=True)\n",
        "            country = location.raw['address']['country']\n",
        "            countries.add(country)\n",
        "        except:\n",
        "            pass\n",
        "    return countries\n",
        "\n",
        "# Example text to infer country from\n",
        "text = \"I am traveling to Paris next month.\"\n",
        "\n",
        "# Extract location names from the text\n",
        "locations = extract_locations(text)\n",
        "\n",
        "# Map location names to countries\n",
        "countries = map_locations_to_countries(locations)\n",
        "\n",
        "# Print the inferred countries\n",
        "print(countries)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0rOTIhWcS8t",
        "outputId": "35f6aed3-7bc4-4c0a-bb74-257277b5c0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities: [('Paris', 'GPE'), ('next month', 'DATE')]\n",
            "{'France'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#traint = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ', traint)\n",
        "#testt = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ', testt)\n",
        "\n",
        "#train[\"new_text\"] = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ', train[\"text\"].apply)\n",
        "#test[\"new_text\"] = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ', test[\"text\"].apply)\n",
        "\n",
        "#newtrain =re.sub(r'<.*?>',' ',newtrain) #removal of html tags\n",
        "#newtest =re.sub(r'<.*?>',' ',newtest)\n",
        "\n",
        "#newtrain = re.sub(r'[^a-zA-Z]',' ',newtrain) #removal of punctuation\n",
        "#newtest = re.sub(r'[^a-zA-Z]',' ',newtest)\n",
        "\n",
        "#newtrain = re.sub(\"[\"\n",
        "#                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n",
        "#                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "#                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "#                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "#                           u\"\\U00002702-\\U000027B0\"\n",
        "#                           u\"\\U000024C2-\\U0001F251\"\n",
        "#                           \"]+\",' ',newtrain)\n",
        "#newtest = re.sub(\"[\"\n",
        "#                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n",
        "#                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "#                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "#                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "#                           u\"\\U00002702-\\U000027B0\"\n",
        "#                           u\"\\U000024C2-\\U0001F251\"\n",
        "#                           \"]+\",' ',newtest)\n",
        "\n",
        "#newtrain = newtrain.lower()\n",
        "#newtest = newtest.lower()\n",
        "\n",
        "#newtrain.head()\n",
        "\n",
        "#newtest.head()\n",
        "\n",
        "#token = Tokenizer()"
      ],
      "metadata": {
        "id": "nEwar0sQkEcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the training and test data\n",
        "tokenizer.fit_on_texts(train_tokens + test_tokens)\n",
        "\n",
        "# Convert the tokens to sequences (list of indices)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_tokens)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_tokens)\n",
        "\n",
        "# Set the max sequence length (you can modify this based on your analysis of sequence lengths)\n",
        "max_timesteps = 100  # Adjust based on your dataset\n",
        "\n",
        "# Pad the sequences\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_timesteps, padding='post', truncating='post', dtype='int32')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_timesteps, padding='post', truncating='post', dtype='int32')\n",
        "\n",
        "# Check the shapes of the padded sequences\n",
        "print(\"Train Padded Shape:\", train_padded.shape)\n",
        "print(\"Test Padded Shape:\", test_padded.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train a Word2Vec model on the tokenized training data\n",
        "word2vec_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "# Save the model for later use\n",
        "word2vec_model.save(\"word2vec_model.model\")\n",
        "\n",
        "\n",
        "# Function to get embeddings for the padded sequences\n",
        "def get_word_embeddings_for_sequences(sequences, word2vec_model, embedding_dim):\n",
        "    embeddings = []\n",
        "    for sequence in sequences:\n",
        "        sequence_embeddings = []\n",
        "        for word_index in sequence:\n",
        "            # Convert word_index to integer to avoid the numpy.float32 issue\n",
        "            word_index = int(word_index)\n",
        "\n",
        "            # Skip padding (0) tokens\n",
        "            if word_index != 0:\n",
        "                # Convert word index to the corresponding word in the vocabulary\n",
        "                # Use index_to_key instead of index2word\n",
        "                if word_index < len(word2vec_model.wv.index_to_key):\n",
        "                    word = word2vec_model.wv.index_to_key[word_index]\n",
        "                    if word in word2vec_model.wv:\n",
        "                        word_embedding = word2vec_model.wv[word]\n",
        "                    else:\n",
        "                        word_embedding = np.zeros(embedding_dim)  # Use zero vector if the word is not in vocabulary\n",
        "                else:\n",
        "                    word_embedding = np.zeros(embedding_dim)  # Use zero vector if index is out of vocabulary range\n",
        "            else:\n",
        "                word_embedding = np.zeros(embedding_dim)  # Padding is represented as a zero vector\n",
        "\n",
        "            sequence_embeddings.append(word_embedding)\n",
        "        embeddings.append(sequence_embeddings)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Now, convert the padded sequences into word embeddings\n",
        "embedding_dim = 100  # As per your Word2Vec model\n",
        "train_embeddings = get_word_embeddings_for_sequences(train_padded, word2vec_model, embedding_dim)\n",
        "test_embeddings = get_word_embeddings_for_sequences(test_padded, word2vec_model, embedding_dim)\n",
        "\n",
        "# Check the shapes of the embeddings\n",
        "print(\"Shape of train embeddings:\", train_embeddings.shape)\n",
        "print(\"Shape of test embeddings:\", test_embeddings.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Generate sentence embeddings for the train and test sets\n",
        "#train_embeddings = [get_sentence_embedding(review, word2vec_model) for review in train_tokens]\n",
        "#test_embeddings = [get_sentence_embedding(review, word2vec_model) for review in test_tokens]\n",
        "\n",
        "\n",
        "# Pad the sequences to ensure uniform length\n",
        "max_sequence_length = max([len(seq) for seq in train_embeddings])  # Max length in the training set\n",
        "\n",
        "train_embeddings_array = np.array(train_embeddings)\n",
        "test_embeddings_array = np.array(test_embeddings)\n",
        "\n",
        "# Now reshape the arrays\n",
        "#train_embeddings_reshaped = train_embeddings_array.reshape((train_embeddings_array.shape[0], 1, 100))  # (samples, timesteps, features)\n",
        "#test_embeddings_reshaped = test_embeddings_array.reshape((test_embeddings_array.shape[0], 1, 100))\n",
        "\n",
        "\n",
        "\n",
        "# Now, let's reshape the padded sequences to match the expected LSTM input format\n",
        "embedding_dim = 100  # Word2Vec embedding dimension\n",
        "\n",
        "# Now, let's map the padded sequences to Word2Vec embeddings\n",
        "train_embeddings = get_word_embeddings_for_sequences(train_padded, word2vec_model, embedding_dim)\n",
        "test_embeddings = get_word_embeddings_for_sequences(test_padded, word2vec_model, embedding_dim)\n",
        "\n",
        "# Reshape the data for LSTM: (samples, timesteps, features)\n",
        "# train_embeddings and test_embeddings should already be of shape (samples, timesteps, embedding_dim)\n",
        "print(\"Train Embeddings Shape:\", train_embeddings.shape)\n",
        "print(\"Test Embeddings Shape:\", test_embeddings.shape)\n",
        "\n",
        "# Reshaping the data for LSTM (samples, timesteps, features)\n",
        "#train_padded_reshaped = train_padded.reshape((train_padded.shape[0], train_padded.shape[1], 100))  # (samples, timesteps, features)\n",
        "#test_padded_reshaped = test_padded.reshape((test_padded.shape[0], test_padded.shape[1], 100))  # (samples, timesteps, features)\n",
        "\n",
        "# Now you can pass this reshaped data into the LSTM model\n",
        "#print(\"Shape of reshaped train data:\", train_padded_reshaped.shape)\n",
        "#print(\"Shape of reshaped test data:\", test_padded_reshaped.shape)\n",
        "\n",
        "\n",
        "# Update the LSTM layer's input shape to match the correct dimensions\n",
        "#model = Sequential()\n",
        "#model.add(LSTM(128, input_shape=(max_sequence_length, word2vec_model.vector_size), return_sequences=False))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(64, activation='relu'))\n",
        "#model.add(Dense(1, activation='sigmoid'))  # Binary classification (for example)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(train_embeddings.shape[1], embedding_dim), return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "#model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Pad the sequences to ensure uniform length\n",
        "max_sequence_length = max([len(seq) for seq in train_embeddings])  # Max length in the training set\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "#model.fit(train_embeddings_reshaped, y_final, epochs=10, batch_size=32, validation_split=0.2)\n",
        "model.fit(train_embeddings, y_final, epochs=5, batch_size=64)\n",
        "\n",
        "y_pred = model.predict(test_embeddings)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Get predictions on the test data\n",
        "#predictions_prob = model.predict(test_embeddings_reshaped)\n",
        "\n",
        "\n",
        "# Convert probabilities to class labels (0 or 1)\n",
        "#predictions = (predictions_prob > 0.5).astype(int)\n",
        "\n",
        "# Show the first few predictions\n",
        "print(y_pred[:10])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "ewaOit2FXXC-",
        "outputId": "2d824857-5d98-4f34-9fe7-1cef49c00e45"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Padded Shape: (7613, 100)\n",
            "Test Padded Shape: (3263, 100)\n",
            "Shape of train embeddings: (7613, 100, 100)\n",
            "Shape of test embeddings: (3263, 100, 100)\n",
            "Train Embeddings Shape: (7613, 100, 100)\n",
            "Test Embeddings Shape: (3263, 100, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m125,569\u001b[0m (490.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,569</span> (490.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m125,569\u001b[0m (490.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,569</span> (490.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/5\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 171ms/step - accuracy: 0.5726 - loss: 0.6899\n",
            "Epoch 2/5\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 161ms/step - accuracy: 0.6218 - loss: 0.6476\n",
            "Epoch 3/5\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 167ms/step - accuracy: 0.6297 - loss: 0.6412\n",
            "Epoch 4/5\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 173ms/step - accuracy: 0.6299 - loss: 0.6434\n",
            "Epoch 5/5\n",
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 166ms/step - accuracy: 0.6213 - loss: 0.6468\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, GlobalMaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define a dictionary of disaster types (example)\n",
        "disaster_keywords = {\n",
        "    'earthquake': ['earthquake', '#earthquake'],\n",
        "    'flood': ['flood', '#flood'],\n",
        "    'fire': ['fire', '#fire'],\n",
        "    'storm': ['storm', '#storm'],\n",
        "    # Add more disaster types as needed\n",
        "}\n",
        "\n",
        "def get_disaster_type(text):\n",
        "    \"\"\" Return the type of disaster based on the tweet's content \"\"\"\n",
        "    for disaster, keywords in disaster_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword.lower() in text.lower():\n",
        "                return disaster\n",
        "    return 'other'  # Default if no match\n",
        "\n",
        "# Add a column for disaster type\n",
        "train['disaster_type'] = train['text'].apply(get_disaster_type)\n",
        "\n",
        "# Convert the disaster_type column to numeric labels for multi-class classification\n",
        "disaster_types = train['disaster_type'].unique()\n",
        "disaster_type_dict = {disaster: idx for idx, disaster in enumerate(disaster_types)}\n",
        "train['disaster_type_label'] = train['disaster_type'].map(disaster_type_dict)\n",
        "\n",
        "\n",
        "# Define input layer for the model\n",
        "input_layer = Input(shape=(x_final.shape[1],))\n",
        "\n",
        "# Embedding layer for word embeddings (we'll assume you have pre-trained word embeddings or are training your own)\n",
        "embedding_layer = Embedding(input_dim=len(token.word_index) + 1, output_dim=100, input_length=x_final.shape[1])(input_layer)\n",
        "\n",
        "# LSTM layer for sequence learning\n",
        "lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)\n",
        "lstm_layer = Dropout(0.2)(lstm_layer)\n",
        "lstm_layer = GlobalMaxPooling1D()(lstm_layer)\n",
        "\n",
        "# Binary classification output (Disaster or not)\n",
        "binary_output = Dense(1, activation='sigmoid', name='binary_output')(lstm_layer)\n",
        "\n",
        "# Multi-class classification output (Type of disaster)\n",
        "disaster_output = Dense(len(disaster_types), activation='softmax', name='disaster_output')(lstm_layer)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_layer, outputs=[binary_output, disaster_output])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss={'binary_output': 'binary_crossentropy', 'disaster_output': 'categorical_crossentropy'},\n",
        "              metrics={'binary_output': 'accuracy', 'disaster_output': 'accuracy'})\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Convert the disaster labels to categorical for multi-class classification\n",
        "y_disaster_type = to_categorical(y_final, num_classes=len(disaster_types))\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_final, [y_final, y_disaster_type], epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Convert test labels to categorical for multi-class classification\n",
        "y_test_disaster_type = to_categorical(y_test_final, num_classes=len(disaster_types))\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(x_test_final, [y_test_final, y_test_disaster_type])\n",
        "\n",
        "# Predict on the test data\n",
        "binary_pred, disaster_pred = model.predict(x_test_final)\n",
        "\n",
        "# Convert binary predictions to labels\n",
        "binary_pred_labels = (binary_pred > 0.5).astype(int)\n",
        "\n",
        "# Convert disaster predictions to labels\n",
        "disaster_pred_labels = disaster_pred.argmax(axis=-1)\n",
        "\n",
        "# Example of predicted values\n",
        "print(binary_pred_labels[:10])\n",
        "print(disaster_pred_labels[:10])\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "JbobFabELDhc",
        "outputId": "dcd3914c-c3a8-4c78-d902-f4dab750a299"
      },
      "execution_count": 19,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_14\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_14\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14486</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14486</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,448,700</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14486</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14486</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ binary_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ disaster_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ global_max_pooling1d_… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14486\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14486\u001b[0m, \u001b[38;5;34m100\u001b[0m)     │      \u001b[38;5;34m1,448,700\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14486\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │        \u001b[38;5;34m117,248\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14486\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ binary_output (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m129\u001b[0m │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ disaster_output (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m645\u001b[0m │ global_max_pooling1d_… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,566,722</span> (5.98 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,566,722\u001b[0m (5.98 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,566,722</span> (5.98 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,566,722\u001b[0m (5.98 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6715s\u001b[0m 70s/step - binary_output_accuracy: 0.5709 - binary_output_loss: 0.6886 - disaster_output_accuracy: 0.4127 - disaster_output_loss: 1.3775 - loss: 2.0662 - val_binary_output_accuracy: 0.5345 - val_binary_output_loss: 0.6928 - val_disaster_output_accuracy: 0.5345 - val_disaster_output_loss: 0.7654 - val_loss: 1.4579\n",
            "Epoch 2/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6710s\u001b[0m 70s/step - binary_output_accuracy: 0.5886 - binary_output_loss: 0.6776 - disaster_output_accuracy: 0.5886 - disaster_output_loss: 0.7081 - loss: 1.3858 - val_binary_output_accuracy: 0.5345 - val_binary_output_loss: 0.6909 - val_disaster_output_accuracy: 0.5345 - val_disaster_output_loss: 0.7266 - val_loss: 1.4173\n",
            "Epoch 3/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6637s\u001b[0m 69s/step - binary_output_accuracy: 0.5761 - binary_output_loss: 0.6819 - disaster_output_accuracy: 0.5761 - disaster_output_loss: 0.6957 - loss: 1.3776 - val_binary_output_accuracy: 0.5345 - val_binary_output_loss: 0.6917 - val_disaster_output_accuracy: 0.5345 - val_disaster_output_loss: 0.7175 - val_loss: 1.4088\n",
            "Epoch 4/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6710s\u001b[0m 69s/step - binary_output_accuracy: 0.5808 - binary_output_loss: 0.6804 - disaster_output_accuracy: 0.5808 - disaster_output_loss: 0.6898 - loss: 1.3702 - val_binary_output_accuracy: 0.5345 - val_binary_output_loss: 0.6913 - val_disaster_output_accuracy: 0.5345 - val_disaster_output_loss: 0.7113 - val_loss: 1.4023\n",
            "Epoch 5/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6767s\u001b[0m 71s/step - binary_output_accuracy: 0.5741 - binary_output_loss: 0.6826 - disaster_output_accuracy: 0.5741 - disaster_output_loss: 0.6897 - loss: 1.3723 - val_binary_output_accuracy: 0.5345 - val_binary_output_loss: 0.6917 - val_disaster_output_accuracy: 0.5345 - val_disaster_output_loss: 0.7085 - val_loss: 1.3999\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_test_final' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4d8e0e000d34>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Convert test labels to categorical for multi-class classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0my_test_disaster_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisaster_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test_final' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary of disaster types (example)\n",
        "disaster_keywords = {\n",
        "    'earthquake': ['earthquake', '#earthquake'],\n",
        "    'flood': ['flood', '#flood'],\n",
        "    'fire': ['fire', '#fire'],\n",
        "    'storm': ['storm', '#storm'],\n",
        "    'hurricane': ['hurricane', '#hurricane'],\n",
        "    'tornado': ['tornado', '#tornado'],\n",
        "    'tsunami': ['tsunami', '#tsunami'],\n",
        "    'wildfire': ['wildfire', '#wildfire'],\n",
        "    'drought': ['drought', '#drought'],\n",
        "    'avalanche': ['avalanche', '#avalanche'],\n",
        "    # Add more disaster types as needed\n",
        "}\n",
        "\n",
        "def get_disaster_type(text):\n",
        "    \"\"\" Return the type of disaster based on the tweet's content \"\"\"\n",
        "    for disaster, keywords in disaster_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword.lower() in text.lower():\n",
        "                return disaster\n",
        "    return 'other'  # Default if no match\n",
        "\n",
        "# Add a column for disaster type\n",
        "train['disaster_type'] = train['text'].apply(get_disaster_type)\n",
        "\n",
        "# Convert the disaster_type column to numeric labels for multi-class classification\n",
        "disaster_types = train['disaster_type'].unique()\n",
        "disaster_type_dict = {disaster: idx for idx, disaster in enumerate(disaster_types)}\n",
        "train['disaster_type_label'] = train['disaster_type'].map(disaster_type_dict)\n",
        "\n",
        "train.head(100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "SeElZzWATxjj",
        "outputId": "d1cd6b6a-92b8-40d1-dabc-d01abcd5e74a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id   keyword              location  \\\n",
              "0     1       NaN                   NaN   \n",
              "1     4       NaN                   NaN   \n",
              "2     5       NaN                   NaN   \n",
              "3     6       NaN                   NaN   \n",
              "4     7       NaN                   NaN   \n",
              "..  ...       ...                   ...   \n",
              "95  137  accident             Charlotte   \n",
              "96  138  accident       Baton Rouge, LA   \n",
              "97  139  accident        Hagerstown, MD   \n",
              "98  141  accident  Gloucestershire , UK   \n",
              "99  143  accident                   NaN   \n",
              "\n",
              "                                                 text  target  \\\n",
              "0   Our Deeds are the Reason of this #earthquake M...       1   \n",
              "1              Forest fire near La Ronge Sask. Canada       1   \n",
              "2   All residents asked to 'shelter in place' are ...       1   \n",
              "3   13,000 people receive #wildfires evacuation or...       1   \n",
              "4   Just got sent this photo from Ruby #Alaska as ...       1   \n",
              "..                                                ...     ...   \n",
              "95  9 Mile backup on I-77 South...accident blockin...       1   \n",
              "96  Has an accident changed your life? We will hel...       0   \n",
              "97  #BREAKING: there was a deadly motorcycle car a...       1   \n",
              "98  @flowri were you marinading it or was it an ac...       0   \n",
              "99  only had a car for not even a week and got in ...       1   \n",
              "\n",
              "                                         Cleaned_text  Sentiment  \\\n",
              "0                deed reason earthquake allah forgive     0.2732   \n",
              "1                  forest fire near ronge sask canada    -0.3400   \n",
              "2   resident asked shelter place notified officer ...    -0.2960   \n",
              "3   people receive wildfire evacuation order calif...     0.0000   \n",
              "4   sent photo ruby alaska smoke wildfire pours sc...     0.0000   \n",
              "..                                                ...        ...   \n",
              "95  mile backup south accident blocking right lane...    -0.3818   \n",
              "96  accident changed life help determine option fi...     0.6705   \n",
              "97  breaking deadly motorcycle car accident happen...    -0.4767   \n",
              "98                         flowri marinading accident    -0.4767   \n",
              "99   car even week fucking car accident fucking drive    -0.5233   \n",
              "\n",
              "   Sentiment_Label   Location_Test disaster_type  disaster_type_label  \n",
              "0         positive              []    earthquake                    0  \n",
              "1         negative        [Canada]          fire                    1  \n",
              "2         negative              []         other                    2  \n",
              "3          neutral    [California]          fire                    1  \n",
              "4          neutral  [Ruby, Alaska]          fire                    1  \n",
              "..             ...             ...           ...                  ...  \n",
              "95        negative    [NC, NC, NC]         other                    2  \n",
              "96        positive              []         other                    2  \n",
              "97        negative    [Hagerstown]         other                    2  \n",
              "98        negative       [@flowri]         other                    2  \n",
              "99        negative              []         other                    2  \n",
              "\n",
              "[100 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43fbd362-37d6-43e2-96f1-25094934f2f7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>Cleaned_text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentiment_Label</th>\n",
              "      <th>Location_Test</th>\n",
              "      <th>disaster_type</th>\n",
              "      <th>disaster_type_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>deed reason earthquake allah forgive</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>forest fire near ronge sask canada</td>\n",
              "      <td>-0.3400</td>\n",
              "      <td>negative</td>\n",
              "      <td>[Canada]</td>\n",
              "      <td>fire</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "      <td>-0.2960</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "      <td>other</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>people receive wildfire evacuation order calif...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[California]</td>\n",
              "      <td>fire</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>sent photo ruby alaska smoke wildfire pours sc...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>[Ruby, Alaska]</td>\n",
              "      <td>fire</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>137</td>\n",
              "      <td>accident</td>\n",
              "      <td>Charlotte</td>\n",
              "      <td>9 Mile backup on I-77 South...accident blockin...</td>\n",
              "      <td>1</td>\n",
              "      <td>mile backup south accident blocking right lane...</td>\n",
              "      <td>-0.3818</td>\n",
              "      <td>negative</td>\n",
              "      <td>[NC, NC, NC]</td>\n",
              "      <td>other</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>138</td>\n",
              "      <td>accident</td>\n",
              "      <td>Baton Rouge, LA</td>\n",
              "      <td>Has an accident changed your life? We will hel...</td>\n",
              "      <td>0</td>\n",
              "      <td>accident changed life help determine option fi...</td>\n",
              "      <td>0.6705</td>\n",
              "      <td>positive</td>\n",
              "      <td>[]</td>\n",
              "      <td>other</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>139</td>\n",
              "      <td>accident</td>\n",
              "      <td>Hagerstown, MD</td>\n",
              "      <td>#BREAKING: there was a deadly motorcycle car a...</td>\n",
              "      <td>1</td>\n",
              "      <td>breaking deadly motorcycle car accident happen...</td>\n",
              "      <td>-0.4767</td>\n",
              "      <td>negative</td>\n",
              "      <td>[Hagerstown]</td>\n",
              "      <td>other</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>141</td>\n",
              "      <td>accident</td>\n",
              "      <td>Gloucestershire , UK</td>\n",
              "      <td>@flowri were you marinading it or was it an ac...</td>\n",
              "      <td>0</td>\n",
              "      <td>flowri marinading accident</td>\n",
              "      <td>-0.4767</td>\n",
              "      <td>negative</td>\n",
              "      <td>[@flowri]</td>\n",
              "      <td>other</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>143</td>\n",
              "      <td>accident</td>\n",
              "      <td>NaN</td>\n",
              "      <td>only had a car for not even a week and got in ...</td>\n",
              "      <td>1</td>\n",
              "      <td>car even week fucking car accident fucking drive</td>\n",
              "      <td>-0.5233</td>\n",
              "      <td>negative</td>\n",
              "      <td>[]</td>\n",
              "      <td>other</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 11 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43fbd362-37d6-43e2-96f1-25094934f2f7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-43fbd362-37d6-43e2-96f1-25094934f2f7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-43fbd362-37d6-43e2-96f1-25094934f2f7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-21729301-50f8-4d46-b941-c7a1e7f64f43\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21729301-50f8-4d46-b941-c7a1e7f64f43')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-21729301-50f8-4d46-b941-c7a1e7f64f43 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 7613,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3137,\n        \"min\": 1,\n        \"max\": 10873,\n        \"num_unique_values\": 7613,\n        \"samples\": [\n          3796,\n          3185,\n          7769\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 221,\n        \"samples\": [\n          \"injury\",\n          \"nuclear%20reactor\",\n          \"engulfed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3341,\n        \"samples\": [\n          \"Oklahoma\",\n          \"Starling City\",\n          \"Trinidad and Tobago\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7503,\n        \"samples\": [\n          \"Three Homes Demolished in Unrecognized Arab Village - International Middle East Media Center http://t.co/ik8m4Yi9T4\",\n          \"Reid Lake fire prompts campground evacuation order http://t.co/jBODKM6rBU\",\n          \"FAAN orders evacuation of abandoned aircraft at MMA http://t.co/dEvYbnVXGQ via @todayng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cleaned_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6803,\n        \"samples\": [\n          \"least dead million displaced india flood india reliefweb\",\n          \"evahanderek marleyknysh time bus driver held hostage mall parking lot lmfao\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4569557196874852,\n        \"min\": -0.9883,\n        \"max\": 0.973,\n        \"num_unique_values\": 1287,\n        \"samples\": [\n          -0.5754,\n          0.9072\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Location_Test\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disaster_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"hurricane\",\n          \"fire\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disaster_type_label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          8,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "#X = train[\"Cleaned_text\"]\n",
        "#y = train[\"target\"]\n",
        "\n",
        "# Convert text data to TF-IDF representation\n",
        "#vectorizer = TfidfVectorizer(max_features=5000)\n",
        "#X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Now split the data (80% train, 20% validation)\n",
        "#X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(\n",
        "#    X_tfidf, y, test_size=0.2, random_state=42\n",
        "#)\n",
        "\n",
        "# Check the shapes to ensure they match\n",
        "#print(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\n",
        "#print(f\"y_train shape: {y_train.shape}\")\n",
        "#print(f\"X_val_tfidf shape: {X_val_tfidf.shape}\")\n",
        "#print(f\"y_val shape: {y_val.shape}\")\n",
        "\n",
        "\n",
        "# Step 1: Convert text data into TF-IDF numerical representation\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Using top 5000 words as features\n",
        "X_train_tfidf = vectorizer.fit_transform(train[\"Cleaned_text\"])  # Fit on training data\n",
        "X_test_tfidf = vectorizer.transform(test[\"Cleaned_text\"])  # Transform test data\n",
        "\n",
        "# Step 2: Set target values\n",
        "y_train = train[\"target\"]\n",
        "\n",
        "# Step 3: Split training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Length of X_train_tfidf: {X_train_tfidf.shape[0]}\")\n",
        "print(f\"Length of y_train: {len(y_train)}\")\n",
        "print(f\"Length of X_val: {X_val.shape[0]}\")\n",
        "print(f\"Length of y_val: {len(y_val)}\")\n",
        "\n",
        "# Model definition\n",
        "#embedding_feature_vector = 200  # Since we used Word2Vec vectors of dim 200.\n",
        "#model = Sequential()\n",
        "#model.add(Embedding(vocab_size, embedding_feature_vector, input_length=max_length, weights=[word_vector_matrix], trainable=False))\n",
        "#model.add(Dropout(0.35))\n",
        "#model.add(LSTM(200))\n",
        "#model.add(Dropout(0.35))\n",
        "#model.add(Dense(32, activation='relu'))\n",
        "#model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#print(model.summary())\n",
        "\n",
        "# Callbacks\n",
        "#n_epoch = 30\n",
        "#early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='min')\n",
        "\n",
        "# Train the model\n",
        "#history = model.fit(X_train, Y_train, validation_data=(x_valid, y_valid),\n",
        "#                    callbacks=[reduce_lr, early_stop], epochs=n_epoch, batch_size=64)\n",
        "\n",
        "\n",
        "# Step 4: Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))  # Input layer with TF-IDF features\n",
        "model.add(Dropout(0.35))  # Dropout layer for regularization\n",
        "model.add(Dense(128, activation='relu'))  # Hidden layer\n",
        "model.add(Dropout(0.35))  # Dropout layer for regularization\n",
        "model.add(Dense(32, activation='relu'))  # Hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Step 5: Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Step 6: Define callbacks\n",
        "n_epoch = 30\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='min')\n",
        "\n",
        "# Step 7: Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                    callbacks=[reduce_lr, early_stop], epochs=n_epoch, batch_size=64)\n",
        "\n",
        "\n",
        "# Assuming the previous steps have been followed, like training the model and defining the feature vectors\n",
        "\n",
        "# Get TF-IDF features for the test data (this is already done in your pipeline)\n",
        "# predictions from the model\n",
        "predictions = model.predict(X_val_tfidf)\n",
        "\n",
        "# Convert probabilities to binary values (0 or 1)\n",
        "binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Print the first 10 predictions\n",
        "print(\"First 10 predictions on validation set (1 = disaster, 0 = not disaster):\")\n",
        "print(binary_predictions[:10])\n",
        "\n",
        "# If you want to see the actual prediction probabilities (between 0 and 1)\n",
        "print(\"First 10 raw prediction probabilities:\")\n",
        "print(predictions[:10])\n",
        "\n",
        "accuracy = accuracy_score(y_val, binary_predictions)\n",
        "\n",
        "# Output the overall accuracy\n",
        "print(f\"Validation accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "# Convert probabilities to binary values (0 or 1)\n",
        "binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "print(\"First 10 predictions on test set (1 = disaster, 0 = not disaster):\")\n",
        "print(binary_predictions[:10])\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "EoI0X2iiCbbn",
        "outputId": "0d6cacb3-eb44-49cb-c575-46f1e33455e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of X_train_tfidf: 7613\n",
            "Length of y_train: 6090\n",
            "Length of X_val: 1523\n",
            "Length of y_val: 1523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m2,560,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m65,664\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m4,128\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,630,337\u001b[0m (10.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,630,337</span> (10.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,630,337\u001b[0m (10.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,630,337</span> (10.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/30\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 61ms/step - accuracy: 0.6152 - loss: 0.6480 - val_accuracy: 0.7886 - val_loss: 0.4700 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8789 - loss: 0.3162 - val_accuracy: 0.7827 - val_loss: 0.4806 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.9276 - loss: 0.1998 - val_accuracy: 0.7715 - val_loss: 0.5829 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.9574 - loss: 0.1223 - val_accuracy: 0.7702 - val_loss: 0.7132 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.9675 - loss: 0.0919 - val_accuracy: 0.7676 - val_loss: 0.8338 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m95/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9783 - loss: 0.0607\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - accuracy: 0.9782 - loss: 0.0610 - val_accuracy: 0.7663 - val_loss: 0.9054 - learning_rate: 0.0010\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "First 10 predictions on validation set (1 = disaster, 0 = not disaster):\n",
            "[[0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n",
            "First 10 raw prediction probabilities:\n",
            "[[0.11164176]\n",
            " [0.1896803 ]\n",
            " [0.7371236 ]\n",
            " [0.08260752]\n",
            " [0.5143592 ]\n",
            " [0.24037282]\n",
            " [0.17812252]\n",
            " [0.32973415]\n",
            " [0.19511896]\n",
            " [0.8409056 ]]\n",
            "Validation accuracy: 78.86%\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "First 10 predictions on test set (1 = disaster, 0 = not disaster):\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding(train_data,test_data):\n",
        "    tfidf = TfidfVectorizer(\n",
        "          ngram_range=(1, 1), use_idf=True, smooth_idf=True, sublinear_tf=True\n",
        "    )\n",
        "    tf_df_train = tfidf.fit_transform(train_data).toarray()\n",
        "    train_df = pd.DataFrame(tf_df_train,columns=tfidf.get_feature_names_out())\n",
        "    tf_df_test = tfidf.transform(test_data).toarray()\n",
        "    test_df = pd.DataFrame(tf_df_test,columns=tfidf.get_feature_names_out())\n",
        "\n",
        "    return train_df,test_df\n",
        "\n",
        "x_final,x_test_final = encoding(train[\"Cleaned_text\"],test[\"Cleaned_text\"])\n",
        "y_final = np.array(y)\n",
        "\n",
        "x_final.shape,y_final.shape,x_test_final.shape\n",
        "\n",
        "# Dividing the data into training, validation and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "# for bow and tf-idf\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.1, random_state=42, stratify = y_final)\n",
        "X_train, x_valid, Y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify = y_train)\n",
        "x_test_final = x_test_final\n",
        "\n",
        "from keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "embedding_feature_vector = 200 # Since we used glove vector embedding of dim 200.\n",
        "#model = Sequential()\n",
        "#model.add(Embedding(vocab_size,embedding_feature_vector,input_length=max_length,weights = [word_vector_matrix], trainable = False))\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))  # Input layer with TF-IDF features\n",
        "model.add(Dropout(0.35))  # Dropout layer for regularization\n",
        "model.add(Dense(128, activation='relu'))  # Hidden layer\n",
        "model.add(Dropout(0.35))  # Dropout layer for regularization\n",
        "model.add(Dense(32, activation='relu'))  # Hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "from tensorflow.keras.callbacks import *\n",
        "n_epoch = 30\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1,\n",
        "                           mode='min', restore_best_weights=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5,\n",
        "                              verbose=1, mode='min')\n",
        "\n",
        "history = model.fit(X_train,Y_train,validation_data=(x_valid,y_valid),callbacks=[reduce_lr,early_stop],epochs=n_epoch,batch_size= 64)\n",
        "\n",
        "predictions = model.predict(x_valid)\n",
        "\n",
        "# Convert probabilities to binary values (0 or 1)\n",
        "binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Print the first 10 predictions\n",
        "print(\"First 10 predictions on validation set (1 = disaster, 0 = not disaster):\")\n",
        "print(binary_predictions[:10])\n",
        "\n",
        "# If you want to see the actual prediction probabilities (between 0 and 1)\n",
        "print(\"First 10 raw prediction probabilities:\")\n",
        "print(predictions[:10])\n",
        "\n",
        "accuracy = accuracy_score(y_valid, binary_predictions)\n",
        "\n",
        "# Output the overall accuracy\n",
        "print(f\"Validation accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "predictions = model.predict(x_test_final)\n",
        "\n",
        "# Convert probabilities to binary values (0 or 1)\n",
        "binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "print(\"First 10 predictions on test set (1 = disaster, 0 = not disaster):\")\n",
        "print(binary_predictions[:10])"
      ],
      "metadata": {
        "id": "Scxvx9qv3zHA",
        "outputId": "480f39a8-20e7-4425-968d-73b137d12e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m7,417,344\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m65,664\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m4,128\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">7,417,344</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,487,169\u001b[0m (28.56 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,487,169</span> (28.56 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,487,169\u001b[0m (28.56 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,487,169</span> (28.56 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/30\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 157ms/step - accuracy: 0.5850 - loss: 0.6490 - val_accuracy: 0.8134 - val_loss: 0.4410 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 159ms/step - accuracy: 0.8945 - loss: 0.2827 - val_accuracy: 0.7959 - val_loss: 0.4829 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 151ms/step - accuracy: 0.9574 - loss: 0.1266 - val_accuracy: 0.7872 - val_loss: 0.5902 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 161ms/step - accuracy: 0.9717 - loss: 0.0778 - val_accuracy: 0.7799 - val_loss: 0.6649 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 212ms/step - accuracy: 0.9785 - loss: 0.0525 - val_accuracy: 0.7930 - val_loss: 0.7258 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9829 - loss: 0.0393\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 163ms/step - accuracy: 0.9828 - loss: 0.0394 - val_accuracy: 0.7843 - val_loss: 0.7538 - learning_rate: 0.0010\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "First 10 predictions on validation set (1 = disaster, 0 = not disaster):\n",
            "[[1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]]\n",
            "First 10 raw prediction probabilities:\n",
            "[[0.87954366]\n",
            " [0.98503536]\n",
            " [0.42272663]\n",
            " [0.922782  ]\n",
            " [0.086373  ]\n",
            " [0.07861262]\n",
            " [0.8389939 ]\n",
            " [0.07771523]\n",
            " [0.7990984 ]\n",
            " [0.19121674]]\n",
            "Validation accuracy: 81.34%\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
            "First 10 predictions on test set (1 = disaster, 0 = not disaster):\n",
            "[[1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    }
  ]
}